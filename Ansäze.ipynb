{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz1\n",
    "#### Specify Model\n",
    "iowa_model = DecisionTreeRegressor()\n",
    "#### Fit Model\n",
    "iowa_model.fit(X, y)\n",
    "\n",
    "print(\"First in-sample predictions:\", iowa_model.predict(X.head()))\n",
    "print(\"Actual target values for those homes:\", y.head().tolist())\n",
    "\n",
    "\n",
    "#### Import the train_test_split function and uncomment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### fill in and uncomment\n",
    "train_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "#### Specify the model\n",
    "iowa_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "#### Fit iowa_model with the training data.\n",
    "iowa_model.fit(train_X,train_y)\n",
    "\n",
    "\n",
    "#### Predict with all validation observations\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "\n",
    "\n",
    "#### print the top few validation predictions\n",
    "print(val_predictions[:5])\n",
    "#### print the top few actual prices from validation data\n",
    "print(val_y[:5])\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz2\n",
    "#### Read the data\n",
    "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "#### Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "#### Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#### Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#### Function for comparing different models\n",
    "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))\n",
    "\n",
    "#### Fill in the best model\n",
    "best_model = model_3\n",
    "\n",
    "#### Define a model\n",
    "my_model = best_model \n",
    "#### Define a model\n",
    "my_model = best_model\n",
    "\n",
    "#### Fit the model to the training data\n",
    "my_model.fit(X, y)\n",
    "\n",
    "#### Generate test predictions\n",
    "preds_test = my_model.predict(X_test)\n",
    "\n",
    "#### Save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz3\n",
    "#### Import helpful libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### Load the data, and separate the target\n",
    "iowa_file_path = '../input/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "y = home_data.SalePrice\n",
    "\n",
    "#### Create X (After completing the exercise, you can return to modify this line!)\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "\n",
    "#### Select columns corresponding to features, and preview the data\n",
    "X = home_data[features]\n",
    "X.head()\n",
    "\n",
    "#### Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#### Define a random forest model\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "rf_val_predictions = rf_model.predict(val_X)\n",
    "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n",
    "\n",
    "#### To improve accuracy, create a new Random Forest model which you will train on all training data\n",
    "rf_model_on_full_data = RandomForestRegressor(random_state=2)\n",
    "\n",
    "\n",
    "#### fit rf_model_on_full_data on all data from the training data\n",
    "rf_model_on_full_data.fit(X,y)\n",
    "\n",
    "#### path to file you will use for predictions\n",
    "test_data_path = '../input/test.csv'\n",
    "\n",
    "#### read test data file using pandas\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "#### create test_X which comes from test_data but includes only the columns you used for prediction.\n",
    "#### The list of columns is stored in a variable called features\n",
    "test_X = test_data[features]\n",
    "\n",
    "#### make predictions which we will submit. \n",
    "test_preds = rf_model_on_full_data.predict(test_X)\n",
    "test_preds\n",
    "\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': test_preds})\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz4\n",
    "#### Code you have previously used to load data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "#### Path of the file to read\n",
    "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "#### Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "#### Create X\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "#### Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#### Specify Model\n",
    "iowa_model = DecisionTreeRegressor(random_state=1)\n",
    "#### Fit Model\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "#### Make validation predictions and calculate mean absolute error\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
    "\n",
    "#### Using best value for max_leaf_nodes\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
    "iowa_model.fit(train_X, train_y)\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#### Define the model. Set random_state to 1\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "#### fit your model\n",
    "rf_model.fit(train_X,train_y)\n",
    "\n",
    "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
    "rf_val_mae = mean_absolute_error(val_y,rf_model.predict(val_X))\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "#### fit your model\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
    "rf_val_predictions = rf_model.predict(val_X)\n",
    "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz6 \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### Read the data\n",
    "X = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "#### Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice              \n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "#### Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "#### \"Cardinality\" means the number of unique values in a column\n",
    "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "#### Select numeric columns\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "#### Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "#### One-hot encode the data (to shorten the code, we use pandas)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#### Define the model\n",
    "my_model_1 = XGBRegressor(random_state=0) # Your code here\n",
    "\n",
    "#### Fit the model\n",
    "my_model_1.fit(X_train,y_train) # Your code here\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#### Get predictions\n",
    "predictions_1 = my_model_1.predict(X_valid) # Your code here\n",
    "\n",
    "\n",
    "#### Calculate MAE\n",
    "mae_1 = mean_absolute_error(y_valid, predictions_1) # Your code here\n",
    "\n",
    "#### Uncomment to print MAE\n",
    "print(\"Mean Absolute Error:\" , mae_1)\n",
    " \n",
    "#### Define the model\n",
    "my_model_2 = XGBRegressor(n_estimators=1000,learning_rate=0.02, random_state=1) # Your code here\n",
    "\n",
    "#### Fit the model\n",
    "my_model_2.fit(X_train,y_train) # Your code here\n",
    "\n",
    "#### Get predictions\n",
    "predictions_2 = my_model_2.predict(X_valid) # Your code here\n",
    "\n",
    "#### Calculate MAE\n",
    "mae_2 = mean_absolute_error(y_valid, predictions_2) # Your code here\n",
    "\n",
    "#### Uncomment to print MAE\n",
    "print(\"Mean Absolute Error:\" , mae_2)\n",
    "mae_2<mae_1\n",
    "\n",
    "\n",
    "#### Step 3: Break the model\n",
    "\n",
    "#### Define the model\n",
    "my_model_3 = XGBRegressor(n_estimators=10000, early_stopping_rounds=20, random_state=3)\n",
    "\n",
    "#### Fit the model\n",
    "my_model_3.fit(X_train,y_train)# Your code here\n",
    "\n",
    "#### Get predictions\n",
    "predictions_3 = my_model_3.predict(X_valid)\n",
    "\n",
    "#### Calculate MAE\n",
    "mae_3 = mean_absolute_error(y_valid, predictions_3)\n",
    "\n",
    "#### Uncomment to print MAE\n",
    "print(\"Mean Absolute Error:\" , mae_3)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz7\n",
    "\n",
    "\n",
    "#### Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "#### Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "#### \"Cardinality\" means the number of unique values in a column\n",
    "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "#### Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "#### Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#### Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "#### Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "#### Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "#### Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "#### Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "#### Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#### Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "#### Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant') # Your code here\n",
    "\n",
    "#### Preprocessing for categorical data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    \n",
    "]) \n",
    "\n",
    "\n",
    "#### Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "#### Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=20) # Your code here\n",
    "\n",
    "\n",
    "#### Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "#### Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "#### Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "#### Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "#### Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "#### Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "#### Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "#### Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Step 2: Generate test predictions\n",
    "\n",
    "\n",
    "#### Preprocessing of test data, fit model\n",
    "preds_test = my_pipeline.predict(X_test) \n",
    "\n",
    "\n",
    "\n",
    "#### Preprocessing of test data, fit model\n",
    "preds_test = my_pipeline.predict(X_test)\n",
    "\n",
    "#### Save test predictions to file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ansatz 8\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### Read the data\n",
    "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "#### Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "#### To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "#### Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# Fill in the line below: get names of columns with missing values\n",
    "miss_col = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "miss_col\n",
    "\n",
    "# Fill in the lines below: drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(miss_col, axis=1) # 1 axis as in cols\n",
    "reduced_X_valid = X_valid.drop(miss_col, axis=1)\n",
    "\n",
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill in the lines below: imputation\n",
    "____ # Your code here\n",
    "impute=SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(impute.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(impute.transform(X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "Run the next code cell without changes to obtain the MAE for this approach.\n",
    "\n",
    "print(\"MAE (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
    "M\n",
    "\n",
    "# Preprocessed training and validation features\n",
    "final_X_train = reduced_X_train\n",
    "final_X_valid = reduced_X_valid\n",
    "\n",
    "# Define and fit model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(final_X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "preds_valid = model.predict(final_X_valid)\n",
    "print(\"MAE (Your approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))\n",
    "\n",
    "\n",
    "# Fill in the line below: preprocess test data\n",
    "impute_X_test= pd.DataFrame(impute.transform(X_test))\n",
    "impute_X_test.columns = X_test.columns\n",
    "\n",
    "final_X_test = impute_X_test.drop(miss_col, axis=1)\n",
    "\n",
    "# Fill in the line below: get test predictions\n",
    "preds_test = model.predict(final_X_test)\n",
    "\n",
    "\n",
    "# Preprocess test data\n",
    "final_X_test = pd.DataFrame(final_imputer.transform(X_test))\n",
    "\n",
    "# Get test predictions\n",
    "preds_test = model.predict(final_X_test)\n",
    "\n",
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "X_test.index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Ansatz 10\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### Read the data\n",
    "X = pd.read_csv('../input/train.csv', index_col='Id') \n",
    "X_test = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "#### Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "#### To keep things simple, we'll drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "print(cols_with_missing)\n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "#X_test.drop(cols_with_missing, axis=1)\n",
    "#print(X_test.isnull().sum())\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "['LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# Fill in the lines below: drop columns in training and validation data\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "MAE from Approach 1 (Drop categorical variables):\n",
    "17837.82570776256\n",
    "Before jumping into ordinal encoding, we'll investigate the dataset. Specifically, we'll look at the 'Condition2' column. The code cell below prints the unique entries in both the training and validation sets.\n",
    "\n",
    "In [8]:\n",
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())\n",
    "\n",
    "\n",
    "In [11]:\n",
    "#### All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "#### Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "#### Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#### Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "#### Apply ordinal encoder \n",
    "____ #### Your code here\n",
    "oe = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = oe.fit_transform(X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = oe.transform(X_valid[good_label_cols])\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "#### Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "#### Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])\n",
    "\n",
    "[('Street', 2),\n",
    " ('Utilities', 2),\n",
    " ('CentralAir', 2),\n",
    " ('LandSlope', 3),\n",
    " ('PavedDrive', 3),\n",
    " ('LotShape', 4),\n",
    " ('LandContour', 4),\n",
    " ('ExterQual', 4),\n",
    " ('KitchenQual', 4),\n",
    " ('MSZoning', 5),\n",
    " ('LotConfig', 5),\n",
    " ('BldgType', 5),\n",
    " ('ExterCond', 5),\n",
    " ('HeatingQC', 5),\n",
    " ('Condition2', 6),\n",
    " ('RoofStyle', 6),\n",
    " ('Foundation', 6),\n",
    " ('Heating', 6),\n",
    " ('Functional', 6),\n",
    " ('SaleCondition', 6),\n",
    " ('RoofMatl', 7),\n",
    " ('HouseStyle', 8),\n",
    " ('Condition1', 9),\n",
    " ('SaleType', 9),\n",
    " ('Exterior1st', 15),\n",
    " ('Exterior2nd', 16),\n",
    " ('Neighborhood', 25)]\n",
    "\n",
    "#### Fill in the line below: How many categorical variables in the training data\n",
    "#### have cardinality greater than 10?\n",
    "high_cardinality_numcols = 3\n",
    "\n",
    "#### Fill in the line below: How many columns are needed to one-hot encode the \n",
    "#### 'Neighborhood' variable in the training data?\n",
    "num_cols_neighborhood = 25\n",
    "\n",
    "\n",
    "#### Fill in the line below: How many entries are added to the dataset by \n",
    "#### replacing the column with a one-hot encoding?\n",
    "OH_entries_added = 10_000_00-10000\n",
    "\n",
    "#### Fill in the line below: How many entries are added to the dataset by\n",
    "#### replacing the column with an ordinal encoding?\n",
    "label_entries_added = 0\n",
    "\n",
    "\n",
    "#### Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "#### Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#### Use as many lines of code as you need!\n",
    "\n",
    "OHE = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "\n",
    "OH_cols_train = pd.DataFrame(OHE.fit_transform(X_train[low_cardinality_cols])) # Your code here\n",
    "OH_cols_valid = pd.DataFrame(OHE.transform(X_valid[low_cardinality_cols])) # Your code here\n",
    "\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "num_X_train = X_train.drop(object_cols,axis=1)\n",
    "num_X_valid= X_valid.drop(object_cols,axis=1)\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train,OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid,OH_cols_valid], axis=1)\n",
    "\n",
    "\n",
    "#### Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "#### One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "#### Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "#### Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "\n",
    "In [23]:\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
    "\n",
    "\n",
    "In [24]:\n",
    "#### (Optional) Your code here\n",
    "'''from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply ordinal encoder \n",
    "____ # Your code here\n",
    "oe = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = oe.fit_transform(X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = oe.transform(X_valid[good_label_cols])\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(label_X_train, y_train)\n",
    "preds = model.predict(label_X_valid)\n",
    "mae = mean_absolute_error(y_valid, preds)\n",
    "\n",
    "X_new=X_test.copy()\n",
    "X_new=X_new.drop(bad_label_cols,axis=1)\n",
    "\n",
    "\n",
    "num_X = X_new.drop(good_label_cols, axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer()\n",
    "imputed_num_X = pd.DataFrame(si.fit_transform(num_X)) #why fit here? just transform didnt work\n",
    "imputed_num_X.columns = num_X.columns\n",
    "\n",
    "label_X_test = X_test.copy()\n",
    "\n",
    "label_X_test = label_X_test.select_dtypes(exclude=['int64','float64'])\n",
    "\n",
    "label_X_test = label_X_test.drop(bad_label_cols, axis=1)\n",
    "label_X_test.isnull().sum()\n",
    "#label_X_test[good_label_cols] = oe.transform(label_X_test[good_label_cols])'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(drop_X_train, y_train)\n",
    "preds = model.predict(drop_X_valid)\n",
    "mae= mean_absolute_error(y_valid, preds)\n",
    "\n",
    "drop_X_test = X_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si = SimpleImputer()\n",
    "\n",
    "idXt = pd.DataFrame(si.fit_transform(drop_X_test))\n",
    "\n",
    "idXt.columns = drop_X_test.columns\n",
    "\n",
    "predict_test = model.predict(idXt)\n",
    "\n",
    "output = pd.DataFrame ( {'Id':X_test.index, 'SalePrice':predict_test})\n",
    "output.to_csv('submission_nothappy_droppedcatvar_NaNinCatVar_cantimpute.csv', index=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice              \n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "X = train_data[numeric_cols].copy()\n",
    "X_test = test_data[numeric_cols].copy()\n",
    "\n",
    "In [4]:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Average MAE score:\", scores.mean())\n",
    "\n",
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    # Replace this body with your own code\n",
    "    my_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()),\n",
    "        ('model', RandomForestRegressor(n_estimators = n_estimators, random_state=0))\n",
    "    ])\n",
    "    scores = -1 * cross_val_score(my_pipe, X, y,\n",
    "                              cv=3,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "´\n",
    "def get_score(n_estimators):\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()),\n",
    "        ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=3,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "\n",
    "results = {i:get_score(i) for i in range(50,450,50) } # Your code here\n",
    "results\n",
    "\n",
    "results = {}\n",
    "for i in range(1,9):\n",
    "    results[50*i] = get_score(50*i)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()\n",
    "\n",
    "n_estimators_best = 200\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
    "/kaggle/input/30-days-of-ml/train.csv\n",
    "/kaggle/input/30-days-of-ml/test.csv\n",
    "Data import\n",
    "In [2]:\n",
    "train = pd.read_csv(\"/kaggle/input/30-days-of-ml/train.csv\", low_memory=False)\n",
    "test = pd.read_csv(\"/kaggle/input/30-days-of-ml/test.csv\", low_memory=False)\n",
    "train.info(memory_usage=\"deep\")\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 300000 entries, 0 to 299999\n",
    "Data columns (total 26 columns):\n",
    " #   Column  Non-Null Count   Dtype  \n",
    "---  ------  --------------   -----  \n",
    " 0   id      300000 non-null  int64  \n",
    " 1   cat0    300000 non-null  object \n",
    " 2   cat1    300000 non-null  object \n",
    " 3   cat2    300000 non-null  object \n",
    " 4   cat3    300000 non-null  object \n",
    " 5   cat4    300000 non-null  object \n",
    " 6   cat5    300000 non-null  object \n",
    " 7   cat6    300000 non-null  object \n",
    " 8   cat7    300000 non-null  object \n",
    " 9   cat8    300000 non-null  object \n",
    " 10  cat9    300000 non-null  object \n",
    " 11  cont0   300000 non-null  float64\n",
    " 12  cont1   300000 non-null  float64\n",
    " 13  cont2   300000 non-null  float64\n",
    " 14  cont3   300000 non-null  float64\n",
    " 15  cont4   300000 non-null  float64\n",
    " 16  cont5   300000 non-null  float64\n",
    " 17  cont6   300000 non-null  float64\n",
    " 18  cont7   300000 non-null  float64\n",
    " 19  cont8   300000 non-null  float64\n",
    " 20  cont9   300000 non-null  float64\n",
    " 21  cont10  300000 non-null  float64\n",
    " 22  cont11  300000 non-null  float64\n",
    " 23  cont12  300000 non-null  float64\n",
    " 24  cont13  300000 non-null  float64\n",
    " 25  target  300000 non-null  float64\n",
    "dtypes: float64(15), int64(1), object(10)\n",
    "memory usage: 202.6 MB\n",
    "In [3]:\n",
    "test.info(memory_usage=\"deep\")\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 200000 entries, 0 to 199999\n",
    "Data columns (total 25 columns):\n",
    " #   Column  Non-Null Count   Dtype  \n",
    "---  ------  --------------   -----  \n",
    " 0   id      200000 non-null  int64  \n",
    " 1   cat0    200000 non-null  object \n",
    " 2   cat1    200000 non-null  object \n",
    " 3   cat2    200000 non-null  object \n",
    " 4   cat3    200000 non-null  object \n",
    " 5   cat4    200000 non-null  object \n",
    " 6   cat5    200000 non-null  object \n",
    " 7   cat6    200000 non-null  object \n",
    " 8   cat7    200000 non-null  object \n",
    " 9   cat8    200000 non-null  object \n",
    " 10  cat9    200000 non-null  object \n",
    " 11  cont0   200000 non-null  float64\n",
    " 12  cont1   200000 non-null  float64\n",
    " 13  cont2   200000 non-null  float64\n",
    " 14  cont3   200000 non-null  float64\n",
    " 15  cont4   200000 non-null  float64\n",
    " 16  cont5   200000 non-null  float64\n",
    " 17  cont6   200000 non-null  float64\n",
    " 18  cont7   200000 non-null  float64\n",
    " 19  cont8   200000 non-null  float64\n",
    " 20  cont9   200000 non-null  float64\n",
    " 21  cont10  200000 non-null  float64\n",
    " 22  cont11  200000 non-null  float64\n",
    " 23  cont12  200000 non-null  float64\n",
    " 24  cont13  200000 non-null  float64\n",
    "dtypes: float64(14), int64(1), object(10)\n",
    "memory usage: 133.5 MB\n",
    "In [4]:\n",
    "train.head(10)\n",
    "Out[4]:\n",
    "id\tcat0\tcat1\tcat2\tcat3\tcat4\tcat5\tcat6\tcat7\tcat8\t...\tcont5\tcont6\tcont7\tcont8\tcont9\tcont10\tcont11\tcont12\tcont13\ttarget\n",
    "0\t1\tB\tB\tB\tC\tB\tB\tA\tE\tC\t...\t0.400361\t0.160266\t0.310921\t0.389470\t0.267559\t0.237281\t0.377873\t0.322401\t0.869850\t8.113634\n",
    "1\t2\tB\tB\tA\tA\tB\tD\tA\tF\tA\t...\t0.533087\t0.558922\t0.516294\t0.594928\t0.341439\t0.906013\t0.921701\t0.261975\t0.465083\t8.481233\n",
    "2\t3\tA\tA\tA\tC\tB\tD\tA\tD\tA\t...\t0.650609\t0.375348\t0.902567\t0.555205\t0.843531\t0.748809\t0.620126\t0.541474\t0.763846\t8.364351\n",
    "3\t4\tB\tB\tA\tC\tB\tD\tA\tE\tC\t...\t0.668980\t0.239061\t0.732948\t0.679618\t0.574844\t0.346010\t0.714610\t0.540150\t0.280682\t8.049253\n",
    "4\t6\tA\tA\tA\tC\tB\tD\tA\tE\tA\t...\t0.686964\t0.420667\t0.648182\t0.684501\t0.956692\t1.000773\t0.776742\t0.625849\t0.250823\t7.972260\n",
    "5\t7\tA\tB\tA\tC\tB\tD\tA\tE\tG\t...\t0.392432\t0.658169\t0.997473\t0.569874\t0.960864\t0.238050\t0.316065\t0.731729\t0.694719\t8.028558\n",
    "6\t8\tB\tA\tA\tA\tB\tD\tA\tE\tC\t...\t0.396705\t0.273454\t0.824573\t0.656325\t0.677114\t0.808445\t0.615973\t0.631677\t0.283561\t7.811465\n",
    "7\t9\tA\tA\tA\tC\tB\tB\tA\tE\tA\t...\t0.633353\t0.339760\t0.802006\t1.010997\t0.391221\t0.057297\t0.591120\t0.074629\t0.775869\t7.674188\n",
    "8\t10\tA\tB\tA\tC\tB\tD\tA\tE\tG\t...\t0.472564\t0.414036\t0.809142\t1.013301\t0.761183\t1.041711\t0.393960\t0.782381\t0.865610\t8.090095\n",
    "9\t11\tA\tA\tA\tA\tB\tB\tA\tE\tE\t...\t0.425716\t0.233705\t0.493036\t0.353048\t0.334675\t0.085087\t0.230634\t0.636732\t0.291874\t8.446155\n",
    "10 rows × 26 columns\n",
    "\n",
    "EDA\n",
    "In [5]:\n",
    "# Colors to be used for plots\n",
    "colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n",
    "          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n",
    "          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n",
    "          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n",
    "In [6]:\n",
    "# Comparing the datasets length\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "pie = ax.pie([len(train), len(test)],\n",
    "             labels=[\"Train dataset\", \"Test dataset\"],\n",
    "             colors=[\"salmon\", \"teal\"],\n",
    "             textprops={\"fontsize\": 15},\n",
    "             autopct='%1.1f%%')\n",
    "ax.axis(\"equal\")\n",
    "ax.set_title(\"Dataset length comparison\", fontsize=18)\n",
    "fig.set_facecolor('white')\n",
    "plt.show();\n",
    "\n",
    "In [7]:\n",
    "# Statistical description of the train dataset\n",
    "train.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T\n",
    "Out[7]:\n",
    "count\tmean\tstd\tmin\t10%\t25%\t50%\t75%\t90%\tmax\n",
    "id\t300000.0\t250018.576947\t144450.150010\t1.000000\t49932.900000\t124772.500000\t250002.500000\t375226.500000\t450149.100000\t499999.000000\n",
    "cont0\t300000.0\t0.527335\t0.230599\t-0.118039\t0.254242\t0.405965\t0.497053\t0.668060\t0.871943\t1.058443\n",
    "cont1\t300000.0\t0.460926\t0.214003\t-0.069309\t0.242940\t0.310494\t0.427903\t0.615113\t0.789630\t0.887253\n",
    "cont2\t300000.0\t0.490498\t0.253346\t-0.056104\t0.182464\t0.300604\t0.502462\t0.647512\t0.848327\t1.034704\n",
    "cont3\t300000.0\t0.496689\t0.219199\t0.130676\t0.219944\t0.329783\t0.465026\t0.664451\t0.796854\t1.039560\n",
    "cont4\t300000.0\t0.491654\t0.240074\t0.255908\t0.276352\t0.284188\t0.390470\t0.696599\t0.876275\t1.055424\n",
    "cont5\t300000.0\t0.510526\t0.228232\t0.045915\t0.251022\t0.354141\t0.488865\t0.669625\t0.828467\t1.067649\n",
    "cont6\t300000.0\t0.467476\t0.210331\t-0.224689\t0.245036\t0.342873\t0.429383\t0.573383\t0.766758\t1.111552\n",
    "cont7\t300000.0\t0.537119\t0.218140\t0.203763\t0.284645\t0.355825\t0.504661\t0.703441\t0.868613\t1.032837\n",
    "cont8\t300000.0\t0.498456\t0.239920\t-0.260275\t0.264466\t0.332486\t0.439151\t0.606056\t0.909264\t1.040229\n",
    "cont9\t300000.0\t0.474872\t0.218007\t0.117896\t0.198457\t0.306874\t0.434620\t0.614333\t0.819239\t0.982922\n",
    "cont10\t300000.0\t0.474492\t0.255949\t0.048732\t0.102849\t0.276017\t0.459975\t0.691579\t0.793483\t1.055960\n",
    "cont11\t300000.0\t0.473216\t0.222022\t0.052608\t0.190619\t0.308151\t0.433812\t0.642057\t0.756893\t1.071444\n",
    "cont12\t300000.0\t0.494561\t0.247292\t-0.074208\t0.189029\t0.289074\t0.422887\t0.714502\t0.834040\t0.975035\n",
    "cont13\t300000.0\t0.508273\t0.222950\t0.151050\t0.250762\t0.300669\t0.472400\t0.758447\t0.830614\t0.905992\n",
    "target\t300000.0\t8.241979\t0.746555\t0.140329\t7.213027\t7.742071\t8.191373\t8.728634\t9.206474\t10.411992\n",
    "In [8]:\n",
    "# Checking if there are missing values in the datasets\n",
    "train.isna().sum().sum(), test.isna().sum().sum()\n",
    "Out[8]:\n",
    "(0, 0)\n",
    "There are no missing value in the both datasets.\n",
    "\n",
    "Let's check target distribution.\n",
    "\n",
    "In [9]:\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "bars = ax.hist(train[\"target\"],\n",
    "               bins=100,\n",
    "               color=\"palevioletred\",\n",
    "               edgecolor=\"black\")\n",
    "ax.set_title(\"Target distribution\", fontsize=20, pad=15)\n",
    "ax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\n",
    "ax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\n",
    "ax.margins(0.025, 0.12)\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "plt.show();\n",
    "\n",
    "In [10]:\n",
    "print(f\"{(train['target'] < 5).sum() / len(train) * 100:.3f}% of the target values are less than 5\")\n",
    "0.059% of the target values are less than 5\n",
    "The dataset contains categorical and numerical values. Let's see values distribution for these categories.\n",
    "\n",
    "In [11]:\n",
    "# Lists of categorical and numerical feature columns\n",
    "cat_features = [\"cat\" + str(i) for i in range(10)]\n",
    "num_features = [\"cont\" + str(i) for i in range(14)]\n",
    "In [12]:\n",
    "# Combined dataframe containing numerical features only\n",
    "df = pd.concat([train[num_features], test[num_features]], axis=0)\n",
    "columns = df.columns.values\n",
    "\n",
    "# Calculating required amount of rows to display all feature plots\n",
    "cols = 3\n",
    "rows = len(columns) // cols + 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n",
    "\n",
    "# Adding some distance between plots\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "\n",
    "# Plots counter\n",
    "i=0\n",
    "for r in np.arange(0, rows, 1):\n",
    "    for c in np.arange(0, cols, 1):\n",
    "        if i >= len(columns): # If there is no more data columns to make plots from\n",
    "            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n",
    "        else:\n",
    "            # Train data histogram\n",
    "            hist1 = axs[r, c].hist(train[columns[i]].values,\n",
    "                                   range=(df[columns[i]].min(),\n",
    "                                          df[columns[i]].max()),\n",
    "                                   bins=40,\n",
    "                                   color=\"deepskyblue\",\n",
    "                                   edgecolor=\"black\",\n",
    "                                   alpha=0.7,\n",
    "                                   label=\"Train Dataset\")\n",
    "            # Test data histogram\n",
    "            hist2 = axs[r, c].hist(test[columns[i]].values,\n",
    "                                   range=(df[columns[i]].min(),\n",
    "                                          df[columns[i]].max()),\n",
    "                                   bins=40,\n",
    "                                   color=\"palevioletred\",\n",
    "                                   edgecolor=\"black\",\n",
    "                                   alpha=0.7,\n",
    "                                   label=\"Test Dataset\")\n",
    "            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n",
    "            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n",
    "            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n",
    "            axs[r, c].grid(axis=\"y\")\n",
    "            axs[r, c].legend(fontsize=13)\n",
    "                                  \n",
    "        i+=1\n",
    "# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\n",
    "plt.show();\n",
    "\n",
    "In [13]:\n",
    "# Combined dataframe containing categorical features only\n",
    "df = pd.concat([train[cat_features], test[cat_features]], axis=0)\n",
    "columns = df.columns.values\n",
    "\n",
    "# Calculating required amount of rows to display all feature plots\n",
    "cols = 3\n",
    "rows = len(columns) // cols + 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n",
    "\n",
    "# Adding some distance between plots\n",
    "plt.subplots_adjust(hspace = 0.2, wspace=0.25)\n",
    "\n",
    "# Plots counter\n",
    "i=0\n",
    "for r in np.arange(0, rows, 1):\n",
    "    for c in np.arange(0, cols, 1):\n",
    "        if i >= len(cat_features): # If there is no more data columns to make plots from\n",
    "            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n",
    "        else:\n",
    "\n",
    "            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n",
    "            bars_pos = np.arange(0, len(values))\n",
    "            if len(values)<4:\n",
    "                height=0.1\n",
    "            else:\n",
    "                height=0.3\n",
    "\n",
    "            bars1 = axs[r, c].barh(bars_pos+height/2,\n",
    "                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n",
    "                                   height=height,\n",
    "                                   color=\"teal\",\n",
    "                                   edgecolor=\"black\",\n",
    "                                   label=\"Train Dataset\")\n",
    "            bars2 = axs[r, c].barh(bars_pos-height/2,\n",
    "                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n",
    "                                   height=height,\n",
    "                                   color=\"salmon\",\n",
    "                                   edgecolor=\"black\",\n",
    "                                   label=\"Test Dataset\")\n",
    "            y_labels = [str(x) for x in values]\n",
    "\n",
    "            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n",
    "            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n",
    "            axs[r, c].set_yticks(bars_pos)\n",
    "            axs[r, c].set_yticklabels(y_labels)\n",
    "            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n",
    "            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n",
    "            axs[r, c].grid(axis=\"x\")\n",
    "            axs[r, c].legend(fontsize=12)\n",
    "            axs[r, c].margins(0.1, 0.02)\n",
    "                                  \n",
    "        i+=1\n",
    "\n",
    "#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\n",
    "plt.show();\n",
    "\n",
    "Let's check if the datasets have different amount of categories in categorical features.\n",
    "\n",
    "In [14]:\n",
    "# Bars position should be numerical because there will be arithmetical operations with them\n",
    "bars_pos = np.arange(len(cat_features))\n",
    "\n",
    "width=0.3\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# Making two bar objects. One is on the left from bar position and the other one is on the right\n",
    "bars1 = ax.bar(bars_pos-width/2,\n",
    "               train[cat_features].nunique().values,\n",
    "               width=width,\n",
    "               color=\"darkorange\", edgecolor=\"black\")\n",
    "bars2 = ax.bar(bars_pos+width/2,\n",
    "               train[cat_features].nunique().values,\n",
    "               width=width,\n",
    "               color=\"steelblue\", edgecolor=\"black\")\n",
    "ax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\n",
    "ax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\n",
    "ax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\n",
    "ax.set_xticks(bars_pos)\n",
    "ax.set_xticklabels(cat_features, fontsize=12)\n",
    "ax.tick_params(axis=\"y\", labelsize=12)\n",
    "ax.grid(axis=\"y\")\n",
    "plt.margins(0.01, 0.05)\n",
    "\n",
    "In [15]:\n",
    "# Checking if test data doesn't contain categories that are not present in the train dataset\n",
    "for col in cat_features:\n",
    "    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "True\n",
    "So the datasets are pretty well balanced. Let's look at feature correlation.\n",
    "\n",
    "In [16]:\n",
    "# Plot dataframe\n",
    "df = train.drop(\"id\", axis=1)\n",
    "\n",
    "# Encoding categorical features with OrdinalEncoder\n",
    "for col in cat_features:\n",
    "    encoder = OrdinalEncoder()\n",
    "    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n",
    "\n",
    "# Calculatin correlation values\n",
    "df = df.corr().round(2)\n",
    "\n",
    "# Mask to hide upper-right part of plot as it is a duplicate\n",
    "mask = np.zeros_like(df)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Making a plot\n",
    "plt.figure(figsize=(14,14))\n",
    "ax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\n",
    "ax.set_title(\"Feature correlation heatmap\", fontsize=17)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "         rotation_mode=\"anchor\", weight=\"normal\")\n",
    "plt.setp(ax.get_yticklabels(), weight=\"normal\",\n",
    "         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\n",
    "plt.show();\n",
    "\n",
    "As you can see, target column is very weakly correlated with all features.\n",
    "\n",
    "Let's visualize each feature vs target.\n",
    "\n",
    "In [17]:\n",
    "columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n",
    "\n",
    "# Calculating required amount of rows to display all feature plots\n",
    "cols = 4\n",
    "rows = len(columns) // cols + 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n",
    "\n",
    "# Adding some distance between plots\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "\n",
    "i=0\n",
    "for r in np.arange(0, rows, 1):\n",
    "    for c in np.arange(0, cols, 1):\n",
    "        if i >= len(columns):\n",
    "            axs[r, c].set_visible(False)\n",
    "        else:\n",
    "            scatter = axs[r, c].scatter(train[columns[i]].values,\n",
    "                                        train[\"target\"],\n",
    "                                        color=random.choice(colors))\n",
    "            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n",
    "            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n",
    "            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n",
    "                                  \n",
    "        i+=1\n",
    "# plt.suptitle(\"Features vs target\", y=0.99)\n",
    "plt.show();\n",
    "\n",
    "Data preprocessing\n",
    "In [18]:\n",
    "# Encoding categorical features with OrdinalEncoder\n",
    "for col in cat_features:\n",
    "    encoder = OrdinalEncoder()\n",
    "    train[col] = encoder.fit_transform(np.array(train[col]).reshape(-1, 1))\n",
    "    test[col] = encoder.transform(np.array(test[col]).reshape(-1, 1))\n",
    "In [19]:\n",
    "train[cat_features].head()\n",
    "Out[19]:\n",
    "cat0\tcat1\tcat2\tcat3\tcat4\tcat5\tcat6\tcat7\tcat8\tcat9\n",
    "0\t1.0\t1.0\t1.0\t2.0\t1.0\t1.0\t0.0\t4.0\t2.0\t13.0\n",
    "1\t1.0\t1.0\t0.0\t0.0\t1.0\t3.0\t0.0\t5.0\t0.0\t14.0\n",
    "2\t0.0\t0.0\t0.0\t2.0\t1.0\t3.0\t0.0\t3.0\t0.0\t5.0\n",
    "3\t1.0\t1.0\t0.0\t2.0\t1.0\t3.0\t0.0\t4.0\t2.0\t10.0\n",
    "4\t0.0\t0.0\t0.0\t2.0\t1.0\t3.0\t0.0\t4.0\t0.0\t13.0\n",
    "In [20]:\n",
    "test[cat_features].head()\n",
    "Out[20]:\n",
    "cat0\tcat1\tcat2\tcat3\tcat4\tcat5\tcat6\tcat7\tcat8\tcat9\n",
    "0\t1.0\t1.0\t1.0\t2.0\t1.0\t1.0\t0.0\t4.0\t4.0\t8.0\n",
    "1\t0.0\t1.0\t0.0\t2.0\t1.0\t2.0\t0.0\t4.0\t2.0\t7.0\n",
    "2\t1.0\t0.0\t0.0\t0.0\t1.0\t1.0\t0.0\t4.0\t3.0\t10.0\n",
    "3\t1.0\t1.0\t0.0\t2.0\t1.0\t3.0\t0.0\t4.0\t0.0\t13.0\n",
    "4\t1.0\t1.0\t0.0\t2.0\t1.0\t2.0\t0.0\t4.0\t2.0\t5.0\n",
    "In [21]:\n",
    "X = train.drop([\"id\", \"target\"], axis=1)\n",
    "X_test = test.drop([\"id\"], axis=1)\n",
    "y = train[\"target\"]\n",
    "Model training\n",
    "In [22]:\n",
    "# Model hyperparameters\n",
    "xgb_params = {'n_estimators': 10000,\n",
    "              'learning_rate': 0.25,\n",
    "              'subsample': 0.926,\n",
    "              'colsample_bytree': 0.84,\n",
    "              'max_depth': 2,\n",
    "              'booster': 'gbtree', \n",
    "              'reg_lambda': 45.1,\n",
    "              'reg_alpha': 34.9,\n",
    "              'random_state': 42,\n",
    "              'n_jobs': 4}\n",
    "In [23]:\n",
    "%%time\n",
    "# Setting up fold parameters\n",
    "splits = 10\n",
    "skf = KFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating an array of zeros for storing \"out of fold\" predictions\n",
    "oof_preds = np.zeros((X.shape[0],))\n",
    "preds = 0\n",
    "model_fi = 0\n",
    "total_mean_rmse = 0\n",
    "\n",
    "# Generating folds and making training and prediction for each of 10 folds\n",
    "for num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n",
    "    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n",
    "    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n",
    "    \n",
    "    model = XGBRegressor(**xgb_params, tree_method = 'gpu_hist')\n",
    "    model.fit(X_train, y_train,\n",
    "              verbose=False,\n",
    "              # These three parameters will stop training before a model starts overfitting \n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_metric=\"rmse\",\n",
    "              early_stopping_rounds=100,\n",
    "              )\n",
    "    \n",
    "    # Getting mean test data predictions (i.e. devided by number of splits)\n",
    "    preds += model.predict(X_test) / splits\n",
    "    \n",
    "    # Getting mean feature importances (i.e. devided by number of splits)\n",
    "    model_fi += model.feature_importances_ / splits\n",
    "    \n",
    "    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n",
    "    # So in the end it will be completely filled with unseen data predictions.\n",
    "    # It will be used to evaluate hyperparameters performance only.\n",
    "    oof_preds[valid_idx] = model.predict(X_valid)\n",
    "    \n",
    "    # Getting score for a fold model\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n",
    "    print(f\"Fold {num} RMSE: {fold_rmse}\")\n",
    "\n",
    "    # Getting mean score of all fold models (i.e. devided by number of splits)\n",
    "    total_mean_rmse += fold_rmse / splits\n",
    "    \n",
    "print(f\"\\nOverall RMSE: {total_mean_rmse}\")\n",
    "Fold 0 RMSE: 0.7176461965682848\n",
    "Fold 1 RMSE: 0.7176683557441019\n",
    "Fold 2 RMSE: 0.7165683496779088\n",
    "Fold 3 RMSE: 0.7185183727623323\n",
    "Fold 4 RMSE: 0.7228696469303012\n",
    "Fold 5 RMSE: 0.7157511344475371\n",
    "Fold 6 RMSE: 0.7192591272961361\n",
    "Fold 7 RMSE: 0.7198663555036886\n",
    "Fold 8 RMSE: 0.7211330404589632\n",
    "Fold 9 RMSE: 0.7142499760102704\n",
    "\n",
    "Overall RMSE: 0.7183530555399524\n",
    "CPU times: user 1min 16s, sys: 913 ms, total: 1min 17s\n",
    "Wall time: 1min 3s\n",
    "Feature importances\n",
    "In [24]:\n",
    "# Creating a dataframe to be used for plotting\n",
    "df = pd.DataFrame()\n",
    "df[\"Feature\"] = X.columns\n",
    "# Extracting feature importances from the trained model\n",
    "df[\"Importance\"] = model_fi / model_fi.sum()\n",
    "# Sorting the dataframe by feature importance\n",
    "df.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n",
    "In [25]:\n",
    "fig, ax = plt.subplots(figsize=(13, 10))\n",
    "bars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n",
    "               color=\"mediumorchid\", edgecolor=\"black\")\n",
    "ax.set_title(\"Feature importances\", fontsize=30, pad=15)\n",
    "ax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n",
    "ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n",
    "ax.set_yticks(df[\"Feature\"])\n",
    "ax.set_yticklabels(df[\"Feature\"], fontsize=15)\n",
    "ax.tick_params(axis=\"x\", labelsize=15)\n",
    "ax.grid(axis=\"x\")\n",
    "# Adding labels on top\n",
    "ax2 = ax.secondary_xaxis('top')\n",
    "ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n",
    "ax2.tick_params(axis=\"x\", labelsize=15)\n",
    "\n",
    "# Inverting y axis direction so the values are decreasing\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "Predictions submission\n",
    "In [26]:\n",
    "predictions = pd.DataFrame()\n",
    "predictions[\"id\"] = test[\"id\"]\n",
    "predictions[\"target\"] = preds\n",
    "\n",
    "predictions.to_csv('submission.csv', index=False, header=predictions.columns)\n",
    "predictions.head()\n",
    "Out[26]:\n",
    "id\ttarget\n",
    "0\t0\t8.116776\n",
    "1\t5\t8.394709\n",
    "2\t15\t8.392687\n",
    "3\t16\t8.527409\n",
    "4\t17\t8.154411\n",
    "In [ ]:\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}