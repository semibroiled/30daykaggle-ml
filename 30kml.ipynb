{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# 30 Day Kaggle Challenge\n",
                "## Competition Phase\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Initialize Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "source": [
                "#Import the important packages\n",
                "import pandas as pd\n",
                "import os, datetime\n",
                "\n",
                "#Models\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from xgboost import XGBRegressor\n",
                "\n",
                "#Validation Fix\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#Model Quality Fix\n",
                "from sklearn.metrics import mean_squared_error\n",
                "# for root mean squared error rmse\n",
                "#rms = mean_squared_error(y, X, squared= False)\n",
                "\n",
                "#Bundling Fix\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "#NaN Fix\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "#Categorical/Object Data Fix\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "#Plotting\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "source": [
                "#Get and Display current working directory\n",
                "cwd = os.getcwd()\n",
                "\n",
                "#Change Directory to parent (one step up)\n",
                "filepath_elements = cwd.split('/')\n",
                "filepath_elements.pop()\n",
                "filepath= '/'.join(filepath_elements)\n",
                "filepath\n",
                "\n",
                "#Save filenames and path as str in variables\n",
                "datapath = '/30-days-of-ml/'\n",
                "dataname_test = 'test.csv'\n",
                "dataname_train = 'train.csv'\n",
                "dataname_samplesubmit = 'sample_submission.csv'\n",
                "\n",
                "#Read the csv files and initialize as Dataframes\n",
                "data_test = pd.read_csv(filepath+datapath+dataname_test, index_col='id')\n",
                "data_train = pd.read_csv(filepath+datapath+dataname_train, index_col='id')\n",
                "data_samplesubmit = pd.read_csv(filepath+datapath+dataname_samplesubmit, index_col='id')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Display Data for viewing\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 96,
            "source": [
                "data_samplesubmit.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 97,
            "source": [
                "data_train.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "source": [
                "data_test.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "source": [
                "#Getting Columns\n",
                "train_cols = list(data_train.columns)\n",
                "print(train_cols);"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'target']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "source": [
                "train_cats = [col for col in train_cols if 'cat' in col]\n",
                "train_conts = [col for col in train_cols if 'cont' in col]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "source": [
                "test_cols = list(data_test.columns)\n",
                "print(test_cols);"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "source": [
                "test_cats = [col for col in test_cols if 'cat' in col]\n",
                "test_conts = [col for col in test_cols if 'cont' in col]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "source": [
                "#Inspect for NaNs\n",
                "print(data_train.isnull().sum().sum())\n",
                "data_train.isnull().any();"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 104,
            "source": [
                "print(data_test.isnull().sum().sum())\n",
                "data_test.isnull().any();"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 105,
            "source": [
                "#Inspect categorical data for cardinality\n",
                "cardinals_train = list(map(lambda col: data_train[col].nunique(), train_cats))\n",
                "save_cardinals_train = dict(zip(train_cats, cardinals_train))\n",
                "sorted(save_cardinals_train.items(), key=lambda x: x[1]);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "data_train.nunique();\n",
                "data_train[train_cats].nunique();\n",
                "list(map(lambda col: data_train[col].nunique(), train_cats));"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 106,
            "source": [
                "cardinals_test = list(map(lambda col: data_test[col].nunique(), test_cats))\n",
                "save_cardinals_test = dict(zip(train_cats, cardinals_test))\n",
                "sorted(save_cardinals_test.items(), key=lambda x: x[1]);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 107,
            "source": [
                "#filter low cardinality columns\n",
                "filter_limit = 10\n",
                "\n",
                "low_cardinality_categorical_cols = [col for col in data_train.columns if data_train[col].nunique()<filter_limit and data_train[col].dtype == 'object']\n",
                "\n",
                "#filter only numerical columns\n",
                "numerical_cols = train_conts.copy()\n",
                "\n",
                "#get columns to be used \n",
                "selected_cols = low_cardinality_categorical_cols + numerical_cols\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### \"Cardinality\" means the number of unique values in a column\n",
                "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
                "categorical_cols = [cname for cname in X_train_full.columns if\n",
                "                    X_train_full[cname].nunique() < 10 and \n",
                "                    X_train_full[cname].dtype == \"object\"]\n",
                "\n",
                "#### Select numerical columns\n",
                "numerical_cols = [cname for cname in X_train_full.columns if \n",
                "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
                "\n",
                "#### Keep selected columns only\n",
                "my_cols = categorical_cols + numerical_cols\n",
                "X_train = X_train_full[my_cols].copy()\n",
                "X_valid = X_valid_full[my_cols].copy()\n",
                "X_test = X_test_full[my_cols].copy()"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 108,
            "source": [
                "#Get full X and full y from data_train and thus X_test from data_test\n",
                "\n",
                "y_full = data_train.target.copy()\n",
                "\n",
                "X_full = data_train[selected_cols].copy()\n",
                "\n",
                "X_test = data_test[selected_cols].copy()\n",
                "\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2,\n",
                "                                                                random_state=0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 109,
            "source": [
                "#Make your pipelines\n",
                "\n",
                "#Preprocessing for numerical data; no NaNs in data means not used though\n",
                "numerical_transformer = SimpleImputer(strategy='constant')\n",
                "\n",
                "numerical_stategies = ['constant', 'mean', 'median', 'most_frequent']\n",
                "\n",
                "#Preprocessing for categorical data\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frquent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "categorical_strategies = ['most_frequent']\n",
                "\n",
                "#Bundle Preprocessing together => numerical and categorical \n",
                "\n",
                "preprocessor = ColumnTransformer(transformers=[\n",
                "    ('num', numerical_transformer, numerical_cols),\n",
                "    ('cat', categorical_transformer, low_cardinality_categorical_cols)\n",
                "\n",
                "])\n",
                "\n",
                "#Define Model(s)\n",
                "\n",
                "#Models\n",
                "\n",
                "#DecisionTreeRegressor\n",
                "dtr1 = DecisionTreeRegressor(random_state=2)\n",
                "dtr2 = DecisionTreeRegressor(max_leaf_nodes=20, random_state=2)\n",
                "dtr3 = DecisionTreeRegressor(max_leaf_nodes=40, random_state=2)\n",
                "dtr4 = DecisionTreeRegressor(max_leaf_nodes=60, random_state=2)\n",
                "dtr5 = DecisionTreeRegressor(max_leaf_nodes=80, random_state=2)\n",
                "\n",
                "#RandomForestRegressor\n",
                "rfr1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
                "rfr2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "rfr3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
                "rfr4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
                "rfr5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
                "\n",
                "#xgboost \n",
                "\n",
                "xgb1 = XGBRegressor(random_state=0)\n",
                "\n",
                "models = [xgb1, dtr1, dtr2, dtr3, dtr4 , dtr5 , rfr1, rfr2, rfr 3, rfr4, rfr5]\n",
                "\n",
                "#Bundle Preprocessing to Model => Model and Preprocessing\n",
                "\n",
                "pipieline1 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr1)\n",
                "])\n",
                "\n",
                "pipieline2 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr2)\n",
                "])\n",
                "\n",
                "pipieline3 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr3)\n",
                "])\n",
                "\n",
                "pipieline4 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr4)\n",
                "])\n",
                "\n",
                "pipieline5 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr5)\n",
                "])\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "#Function for comparing different models\n",
                "def score_model(my_pipeline, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
                "    my_pipeline.fit(X_t, y_t)\n",
                "    predictions = my_pipeline.predict(X_v)\n",
                "    rms = mean_squared_error(y_v, predictions, squared= False)\n",
                "    return rms\n",
                "\n",
                "'''for i in range(0, len(models)):\n",
                "    mae = score_model(models[i])\n",
                "    print(\"Model %d MAE: %d\" % (i+1, mae))'''\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-109-4724b53932ea>, line 47)",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-109-4724b53932ea>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    models = [xgb1, dtr1, dtr2, dtr3, dtr4 , dtr5 , rfr1, rfr2, rfr 3, rfr4, rfr5]\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "source": [
                "#Select best approach\n",
                "\n",
                "#Generate test predictions\n",
                "\n",
                "#predictions_test = my_pipeline.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Approach 1a: Drop all categorical columns"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#Code snippet\n",
                "#dropna data=data.dropna(axis=0) which i think means rows\n",
                "\n",
                "data_train_1a = data_train.copy()\n",
                "\n",
                "\n",
                "#Gain y from train data\n",
                "y_train_1a = data_train_1a.target\n",
                "\n",
                "#features_nocat = [col for col in train_cols if 'cont' in col]\n",
                "#print(features_nocat)\n",
                "\n",
                "#Gain X from train data\n",
                "X_train_1a = data_train_1a.drop(['target'], axis=1) #inlince keeps one copy and makes changes to it instead of making new copes\n",
                "\n",
                "#alternatively\n",
                "#train_cols.pop('target') => maybe this is a thing? Idk haven't checked\n",
                "# X_train = data_train[train_cols]"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.4 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "5bb92a85c9a1f15da6b689aacf75061d89825faba8bdf21749792174943e74b5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}