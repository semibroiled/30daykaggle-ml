{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# 30 Day Kaggle Challenge\n",
                "## Competition Phase\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Initialize Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "source": [
                "#Import the important packages\n",
                "import pandas as pd\n",
                "import os, datetime\n",
                "\n",
                "#Models\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from xgboost import XGBRegressor\n",
                "\n",
                "#Validation Fix\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#Model Quality Fix\n",
                "from sklearn.metrics import mean_squared_error\n",
                "# for root mean squared error rmse\n",
                "#rms = mean_squared_error(y, X, squared= False)\n",
                "\n",
                "#Bundling Fix\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "#NaN Fix\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "#Categorical/Object Data Fix\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.preprocessing import OrdinalEncoder"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "#Get and Display current working directory\n",
                "cwd = os.getcwd()\n",
                "\n",
                "#Change Directory to parent (one step up)\n",
                "filepath_elements = cwd.split('/')\n",
                "filepath_elements.pop()\n",
                "filepath= '/'.join(filepath_elements)\n",
                "filepath\n",
                "\n",
                "#Save filenames and path as str in variables\n",
                "datapath = '/30-days-of-ml/'\n",
                "dataname_test = 'test.csv'\n",
                "dataname_train = 'train.csv'\n",
                "dataname_samplesubmit = 'sample_submission.csv'\n",
                "\n",
                "#Read the csv files and initialize as Dataframes\n",
                "data_test = pd.read_csv(filepath+datapath+dataname_test, index_col='id')\n",
                "data_train = pd.read_csv(filepath+datapath+dataname_train, index_col='id')\n",
                "data_samplesubmit = pd.read_csv(filepath+datapath+dataname_samplesubmit, index_col='id')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "source": [
                "help(mean_squared_error)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Help on function mean_squared_error in module sklearn.metrics.regression:\n",
                        "\n",
                        "mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
                        "    Mean squared error regression loss\n",
                        "    \n",
                        "    Read more in the :ref:`User Guide <mean_squared_error>`.\n",
                        "    \n",
                        "    Parameters\n",
                        "    ----------\n",
                        "    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
                        "        Ground truth (correct) target values.\n",
                        "    \n",
                        "    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
                        "        Estimated target values.\n",
                        "    \n",
                        "    sample_weight : array-like of shape = (n_samples), optional\n",
                        "        Sample weights.\n",
                        "    \n",
                        "    multioutput : string in ['raw_values', 'uniform_average']\n",
                        "        or array-like of shape (n_outputs)\n",
                        "        Defines aggregating of multiple output values.\n",
                        "        Array-like value defines weights used to average errors.\n",
                        "    \n",
                        "        'raw_values' :\n",
                        "            Returns a full set of errors in case of multioutput input.\n",
                        "    \n",
                        "        'uniform_average' :\n",
                        "            Errors of all outputs are averaged with uniform weight.\n",
                        "    \n",
                        "    Returns\n",
                        "    -------\n",
                        "    loss : float or ndarray of floats\n",
                        "        A non-negative floating point value (the best value is 0.0), or an\n",
                        "        array of floating point values, one for each individual target.\n",
                        "    \n",
                        "    Examples\n",
                        "    --------\n",
                        "    >>> from sklearn.metrics import mean_squared_error\n",
                        "    >>> y_true = [3, -0.5, 2, 7]\n",
                        "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
                        "    >>> mean_squared_error(y_true, y_pred)\n",
                        "    0.375\n",
                        "    >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
                        "    >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
                        "    >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
                        "    0.708...\n",
                        "    >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
                        "    ... # doctest: +ELLIPSIS\n",
                        "    array([0.41666667, 1.        ])\n",
                        "    >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
                        "    ... # doctest: +ELLIPSIS\n",
                        "    0.825...\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Display Data for viewing\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "source": [
                "data_samplesubmit.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "data_train.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "source": [
                "data_test.head(10);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "source": [
                "#Getting Columns\n",
                "train_cols = list(data_train.columns)\n",
                "print(train_cols);"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'target']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "source": [
                "train_cats = [col for col in train_cols if 'cat' in col]\n",
                "train_conts = [col for col in train_cols if 'cont' in col]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "source": [
                "test_cols = list(data_test.columns)\n",
                "print(test_cols);"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "source": [
                "test_cats = [col for col in test_cols if 'cat' in col]\n",
                "test_conts = [col for col in test_cols if 'cont' in col]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "source": [
                "#Inspect for NaNs\n",
                "print(data_train.isnull().sum().sum())\n",
                "data_train.isnull().any();"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "source": [
                "print(data_test.isnull().sum().sum())\n",
                "data_test.isnull().any();"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "source": [
                "#Inspect categorical data for cardinality\n",
                "cardinals_train = list(map(lambda col: data_train[col].nunique(), train_cats))\n",
                "save_cardinals_train = dict(zip(train_cats, cardinals_train))\n",
                "sorted(save_cardinals_train.items(), key=lambda x: x[1]);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "data_train.nunique();\n",
                "data_train[train_cats].nunique();\n",
                "list(map(lambda col: data_train[col].nunique(), train_cats));"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "source": [
                "cardinals_test = list(map(lambda col: data_test[col].nunique(), test_cats))\n",
                "save_cardinals_test = dict(zip(train_cats, cardinals_test))\n",
                "sorted(save_cardinals_test.items(), key=lambda x: x[1]);"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "#filter low cardinality columns\n",
                "filter_limit = 10\n",
                "\n",
                "low_cardinality_categorical_cols = [col for col in data_train.columns if data_train[col].nunique()<filter_limit and data_train[col].dtype == 'object']\n",
                "\n",
                "#filter only numerical columns\n",
                "numerical_cols = train_conts.copy()\n",
                "\n",
                "#get columns to be used \n",
                "selected_cols = low_cardinality_categorical_cols + numerical_cols\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### \"Cardinality\" means the number of unique values in a column\n",
                "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
                "categorical_cols = [cname for cname in X_train_full.columns if\n",
                "                    X_train_full[cname].nunique() < 10 and \n",
                "                    X_train_full[cname].dtype == \"object\"]\n",
                "\n",
                "#### Select numerical columns\n",
                "numerical_cols = [cname for cname in X_train_full.columns if \n",
                "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
                "\n",
                "#### Keep selected columns only\n",
                "my_cols = categorical_cols + numerical_cols\n",
                "X_train = X_train_full[my_cols].copy()\n",
                "X_valid = X_valid_full[my_cols].copy()\n",
                "X_test = X_test_full[my_cols].copy()"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "source": [
                "#Get full X and full y from data_train and thus X_test from data_test\n",
                "\n",
                "y_full = data_train.target.copy()\n",
                "\n",
                "X_full = data_train[selected_cols].copy()\n",
                "\n",
                "X_test = data_test[selected_cols].copy()\n",
                "\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2,\n",
                "                                                                random_state=0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "source": [
                "#Make your pipelines\n",
                "\n",
                "#Preprocessing for numerical data; no NaNs in data means not used though\n",
                "numerical_transformer = SimpleImputer(strategy='constant')\n",
                "\n",
                "numerical_stategies = ['constant', 'mean', 'median', 'most_frequent']\n",
                "\n",
                "#Preprocessing for categorical data\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frquent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "categorical_strategies = ['most_frequent']\n",
                "\n",
                "#Bundle Preprocessing together => numerical and categorical \n",
                "\n",
                "preprocessor = ColumnTransformer(transformers=[\n",
                "    ('num', numerical_transformer, numerical_cols),\n",
                "    ('cat', categorical_transformer, low_cardinality_categorical_cols)\n",
                "\n",
                "])\n",
                "\n",
                "#Define Model(s)\n",
                "\n",
                "#Models\n",
                "\n",
                "#DecisionTreeRegressor\n",
                "dtr1 = DecisionTreeRegressor(random_state=2)\n",
                "dtr2 = DecisionTreeRegressor(max_leaf_nodes=20, random_state=2)\n",
                "dtr3 = DecisionTreeRegressor(max_leaf_nodes=40, random_state=2)\n",
                "dtr4 = DecisionTreeRegressor(max_leaf_nodes=60, random_state=2)\n",
                "dtr5 = DecisionTreeRegressor(max_leaf_nodes=80, random_state=2)\n",
                "\n",
                "#RandomForestRegressor\n",
                "rfr1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
                "rfr2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "rfr3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
                "rfr4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
                "rfr5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
                "\n",
                "#xgboost \n",
                "\n",
                "xgb1 = XGBRegressor(random_state=0)\n",
                "\n",
                "models = [xgb1, dtr1, dtr2, dtr3, dtr4 , dtr5 , rfr1, rfr2, rfr 3, rfr4, rfr5]\n",
                "\n",
                "#Bundle Preprocessing to Model => Model and Preprocessing\n",
                "\n",
                "pipieline1 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr1)\n",
                "])\n",
                "\n",
                "pipieline2 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr2)\n",
                "])\n",
                "\n",
                "pipieline3 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr3)\n",
                "])\n",
                "\n",
                "pipieline4 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr4)\n",
                "])\n",
                "\n",
                "pipieline5 = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('model', dtr5)\n",
                "])\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "#Function for comparing different models\n",
                "def score_model(my_pipeline, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
                "    my_pipeline.fit(X_t, y_t)\n",
                "    predictions = my_pipeline.predict(X_v)\n",
                "    rms = mean_squared_error(y_v, predictions, squared= False)\n",
                "    return rms\n",
                "\n",
                "'''for i in range(0, len(models)):\n",
                "    mae = score_model(models[i])\n",
                "    print(\"Model %d MAE: %d\" % (i+1, mae))'''\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "TypeError",
                    "evalue": "int() argument must be a string, a bytes-like object or a number, not 'range'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-87-85a87deacd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxgb1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dtr{n}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'range'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "source": [
                "#Select best approach\n",
                "\n",
                "#Generate test predictions\n",
                "\n",
                "#predictions_test = my_pipeline.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Approach 1a: Drop all categorical columns"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "#Code snippet\n",
                "#dropna data=data.dropna(axis=0) which i think means rows\n",
                "\n",
                "data_train_1a = data_train.copy()\n",
                "\n",
                "\n",
                "#Gain y from train data\n",
                "y_train_1a = data_train_1a.target\n",
                "\n",
                "#features_nocat = [col for col in train_cols if 'cont' in col]\n",
                "#print(features_nocat)\n",
                "\n",
                "#Gain X from train data\n",
                "X_train_1a = data_train_1a.drop(['target'], axis=1) #inlince keeps one copy and makes changes to it instead of making new copes\n",
                "\n",
                "#alternatively\n",
                "#train_cols.pop('target') => maybe this is a thing? Idk haven't checked\n",
                "# X_train = data_train[train_cols]"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.4 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "5bb92a85c9a1f15da6b689aacf75061d89825faba8bdf21749792174943e74b5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}