{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# 30 Day Kaggle Challenge\n",
                "## Competition Phase\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Initialize Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "#Import thr important packages\n",
                "import pandas as pd\n",
                "import os, datetime\n",
                "\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "from sklearn.metrics import mean_squared_error\n",
                "# for root mean squared error rmse\n",
                "#rms = mean_squared_error(y, X, squared= False)\n",
                "\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "source": [
                "#Get and Display current working directory\n",
                "cwd = os.getcwd()\n",
                "print(cwd)\n",
                "\n",
                "#Change Directory to parent (one step up)\n",
                "filepath_elements = cwd.split('/')\n",
                "filepath_elements.pop()\n",
                "filepath= '/'.join(filepath_elements)\n",
                "filepath\n",
                "\n",
                "#Save filenames and path as str in variables\n",
                "datapath = '/30-days-of-ml/'\n",
                "dataname_test = 'test.csv'\n",
                "dataname_train = 'train.csv'\n",
                "dataname_samplesubmit = 'sample_submission.csv'\n",
                "\n",
                "#Read the csv files and initialize as Dataframes\n",
                "data_test = pd.read_csv(filepath+datapath+dataname_test)\n",
                "data_train = pd.read_csv(filepath+datapath+dataname_train)\n",
                "data_samplesubmit = pd.read_csv(filepath+datapath+dataname_samplesubmit)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "/Users/amitavchrismostafa/Documents/Python/kaggle-project/30daykaggle-ml\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Display Data for viewing\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "source": [
                "data_samplesubmit.head(10)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "   id  target\n",
                            "0   0     0.5\n",
                            "1   5     0.5\n",
                            "2  15     0.5\n",
                            "3  16     0.5\n",
                            "4  17     0.5\n",
                            "5  19     0.5\n",
                            "6  20     0.5\n",
                            "7  21     0.5\n",
                            "8  23     0.5\n",
                            "9  29     0.5"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>target</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>5</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>15</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>16</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>17</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>19</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>20</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>21</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>23</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>29</td>\n",
                            "      <td>0.5</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 82
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "source": [
                "data_train.head(10)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont5     cont6  \\\n",
                            "0   1    B    B    B    C    B    B    A    E    C  ...  0.400361  0.160266   \n",
                            "1   2    B    B    A    A    B    D    A    F    A  ...  0.533087  0.558922   \n",
                            "2   3    A    A    A    C    B    D    A    D    A  ...  0.650609  0.375348   \n",
                            "3   4    B    B    A    C    B    D    A    E    C  ...  0.668980  0.239061   \n",
                            "4   6    A    A    A    C    B    D    A    E    A  ...  0.686964  0.420667   \n",
                            "5   7    A    B    A    C    B    D    A    E    G  ...  0.392432  0.658169   \n",
                            "6   8    B    A    A    A    B    D    A    E    C  ...  0.396705  0.273454   \n",
                            "7   9    A    A    A    C    B    B    A    E    A  ...  0.633353  0.339760   \n",
                            "8  10    A    B    A    C    B    D    A    E    G  ...  0.472564  0.414036   \n",
                            "9  11    A    A    A    A    B    B    A    E    E  ...  0.425716  0.233705   \n",
                            "\n",
                            "      cont7     cont8     cont9    cont10    cont11    cont12    cont13  \\\n",
                            "0  0.310921  0.389470  0.267559  0.237281  0.377873  0.322401  0.869850   \n",
                            "1  0.516294  0.594928  0.341439  0.906013  0.921701  0.261975  0.465083   \n",
                            "2  0.902567  0.555205  0.843531  0.748809  0.620126  0.541474  0.763846   \n",
                            "3  0.732948  0.679618  0.574844  0.346010  0.714610  0.540150  0.280682   \n",
                            "4  0.648182  0.684501  0.956692  1.000773  0.776742  0.625849  0.250823   \n",
                            "5  0.997473  0.569874  0.960864  0.238050  0.316065  0.731729  0.694719   \n",
                            "6  0.824573  0.656325  0.677114  0.808445  0.615973  0.631677  0.283561   \n",
                            "7  0.802006  1.010997  0.391221  0.057297  0.591120  0.074629  0.775869   \n",
                            "8  0.809142  1.013301  0.761183  1.041711  0.393960  0.782381  0.865610   \n",
                            "9  0.493036  0.353048  0.334675  0.085087  0.230634  0.636732  0.291874   \n",
                            "\n",
                            "     target  \n",
                            "0  8.113634  \n",
                            "1  8.481233  \n",
                            "2  8.364351  \n",
                            "3  8.049253  \n",
                            "4  7.972260  \n",
                            "5  8.028558  \n",
                            "6  7.811465  \n",
                            "7  7.674188  \n",
                            "8  8.090095  \n",
                            "9  8.446155  \n",
                            "\n",
                            "[10 rows x 26 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>cat0</th>\n",
                            "      <th>cat1</th>\n",
                            "      <th>cat2</th>\n",
                            "      <th>cat3</th>\n",
                            "      <th>cat4</th>\n",
                            "      <th>cat5</th>\n",
                            "      <th>cat6</th>\n",
                            "      <th>cat7</th>\n",
                            "      <th>cat8</th>\n",
                            "      <th>...</th>\n",
                            "      <th>cont5</th>\n",
                            "      <th>cont6</th>\n",
                            "      <th>cont7</th>\n",
                            "      <th>cont8</th>\n",
                            "      <th>cont9</th>\n",
                            "      <th>cont10</th>\n",
                            "      <th>cont11</th>\n",
                            "      <th>cont12</th>\n",
                            "      <th>cont13</th>\n",
                            "      <th>target</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>C</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.400361</td>\n",
                            "      <td>0.160266</td>\n",
                            "      <td>0.310921</td>\n",
                            "      <td>0.389470</td>\n",
                            "      <td>0.267559</td>\n",
                            "      <td>0.237281</td>\n",
                            "      <td>0.377873</td>\n",
                            "      <td>0.322401</td>\n",
                            "      <td>0.869850</td>\n",
                            "      <td>8.113634</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>F</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.533087</td>\n",
                            "      <td>0.558922</td>\n",
                            "      <td>0.516294</td>\n",
                            "      <td>0.594928</td>\n",
                            "      <td>0.341439</td>\n",
                            "      <td>0.906013</td>\n",
                            "      <td>0.921701</td>\n",
                            "      <td>0.261975</td>\n",
                            "      <td>0.465083</td>\n",
                            "      <td>8.481233</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>3</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.650609</td>\n",
                            "      <td>0.375348</td>\n",
                            "      <td>0.902567</td>\n",
                            "      <td>0.555205</td>\n",
                            "      <td>0.843531</td>\n",
                            "      <td>0.748809</td>\n",
                            "      <td>0.620126</td>\n",
                            "      <td>0.541474</td>\n",
                            "      <td>0.763846</td>\n",
                            "      <td>8.364351</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>4</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>C</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.668980</td>\n",
                            "      <td>0.239061</td>\n",
                            "      <td>0.732948</td>\n",
                            "      <td>0.679618</td>\n",
                            "      <td>0.574844</td>\n",
                            "      <td>0.346010</td>\n",
                            "      <td>0.714610</td>\n",
                            "      <td>0.540150</td>\n",
                            "      <td>0.280682</td>\n",
                            "      <td>8.049253</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>6</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.686964</td>\n",
                            "      <td>0.420667</td>\n",
                            "      <td>0.648182</td>\n",
                            "      <td>0.684501</td>\n",
                            "      <td>0.956692</td>\n",
                            "      <td>1.000773</td>\n",
                            "      <td>0.776742</td>\n",
                            "      <td>0.625849</td>\n",
                            "      <td>0.250823</td>\n",
                            "      <td>7.972260</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>7</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>G</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.392432</td>\n",
                            "      <td>0.658169</td>\n",
                            "      <td>0.997473</td>\n",
                            "      <td>0.569874</td>\n",
                            "      <td>0.960864</td>\n",
                            "      <td>0.238050</td>\n",
                            "      <td>0.316065</td>\n",
                            "      <td>0.731729</td>\n",
                            "      <td>0.694719</td>\n",
                            "      <td>8.028558</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>8</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>C</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.396705</td>\n",
                            "      <td>0.273454</td>\n",
                            "      <td>0.824573</td>\n",
                            "      <td>0.656325</td>\n",
                            "      <td>0.677114</td>\n",
                            "      <td>0.808445</td>\n",
                            "      <td>0.615973</td>\n",
                            "      <td>0.631677</td>\n",
                            "      <td>0.283561</td>\n",
                            "      <td>7.811465</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>9</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.633353</td>\n",
                            "      <td>0.339760</td>\n",
                            "      <td>0.802006</td>\n",
                            "      <td>1.010997</td>\n",
                            "      <td>0.391221</td>\n",
                            "      <td>0.057297</td>\n",
                            "      <td>0.591120</td>\n",
                            "      <td>0.074629</td>\n",
                            "      <td>0.775869</td>\n",
                            "      <td>7.674188</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>10</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>G</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.472564</td>\n",
                            "      <td>0.414036</td>\n",
                            "      <td>0.809142</td>\n",
                            "      <td>1.013301</td>\n",
                            "      <td>0.761183</td>\n",
                            "      <td>1.041711</td>\n",
                            "      <td>0.393960</td>\n",
                            "      <td>0.782381</td>\n",
                            "      <td>0.865610</td>\n",
                            "      <td>8.090095</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>11</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>E</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.425716</td>\n",
                            "      <td>0.233705</td>\n",
                            "      <td>0.493036</td>\n",
                            "      <td>0.353048</td>\n",
                            "      <td>0.334675</td>\n",
                            "      <td>0.085087</td>\n",
                            "      <td>0.230634</td>\n",
                            "      <td>0.636732</td>\n",
                            "      <td>0.291874</td>\n",
                            "      <td>8.446155</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>10 rows × 26 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 83
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "source": [
                "data_test.head(10)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont4     cont5  \\\n",
                            "0   0    B    B    B    C    B    B    A    E    E  ...  0.476739  0.376350   \n",
                            "1   5    A    B    A    C    B    C    A    E    C  ...  0.285509  0.860046   \n",
                            "2  15    B    A    A    A    B    B    A    E    D  ...  0.697272  0.683600   \n",
                            "3  16    B    B    A    C    B    D    A    E    A  ...  0.719306  0.777890   \n",
                            "4  17    B    B    A    C    B    C    A    E    C  ...  0.313032  0.431007   \n",
                            "5  19    B    B    B    C    B    B    A    E    E  ...  0.489711  0.533519   \n",
                            "6  20    A    A    A    C    B    D    A    E    G  ...  0.285503  0.497190   \n",
                            "7  21    B    B    A    C    B    D    A    E    A  ...  0.346929  0.774717   \n",
                            "8  23    A    A    A    C    B    B    A    E    E  ...  0.484546  0.688881   \n",
                            "9  29    A    A    A    C    B    D    A    E    A  ...  0.425122  0.758756   \n",
                            "\n",
                            "      cont6     cont7     cont8     cont9    cont10    cont11    cont12  \\\n",
                            "0  0.337884  0.321832  0.445212  0.290258  0.244476  0.087914  0.301831   \n",
                            "1  0.798712  0.835961  0.391657  0.288276  0.549568  0.905097  0.850684   \n",
                            "2  0.404089  0.879379  0.275549  0.427871  0.491667  0.384315  0.376689   \n",
                            "3  0.730954  0.644315  1.024017  0.391090  0.988340  0.411828  0.393585   \n",
                            "4  0.390992  0.408874  0.447887  0.390253  0.648932  0.385935  0.370401   \n",
                            "5  0.451563  0.513817  0.429349  0.489093  0.326913  0.440908  0.694076   \n",
                            "6  0.540631  0.521383  0.277155  0.837442  0.602099  1.057592  0.522523   \n",
                            "7  0.374756  0.838619  0.641282  0.854445  0.834690  0.347269  0.741068   \n",
                            "8  0.356810  0.732911  0.259619  0.275222  0.750423  0.783911  0.601736   \n",
                            "9  0.706094  0.871956  0.584765  0.550189  0.775375  0.798391  0.972790   \n",
                            "\n",
                            "     cont13  \n",
                            "0  0.845702  \n",
                            "1  0.693940  \n",
                            "2  0.508099  \n",
                            "3  0.461372  \n",
                            "4  0.900412  \n",
                            "5  0.444859  \n",
                            "6  0.668214  \n",
                            "7  0.257963  \n",
                            "8  0.811545  \n",
                            "9  0.283684  \n",
                            "\n",
                            "[10 rows x 25 columns]"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>cat0</th>\n",
                            "      <th>cat1</th>\n",
                            "      <th>cat2</th>\n",
                            "      <th>cat3</th>\n",
                            "      <th>cat4</th>\n",
                            "      <th>cat5</th>\n",
                            "      <th>cat6</th>\n",
                            "      <th>cat7</th>\n",
                            "      <th>cat8</th>\n",
                            "      <th>...</th>\n",
                            "      <th>cont4</th>\n",
                            "      <th>cont5</th>\n",
                            "      <th>cont6</th>\n",
                            "      <th>cont7</th>\n",
                            "      <th>cont8</th>\n",
                            "      <th>cont9</th>\n",
                            "      <th>cont10</th>\n",
                            "      <th>cont11</th>\n",
                            "      <th>cont12</th>\n",
                            "      <th>cont13</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>E</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.476739</td>\n",
                            "      <td>0.376350</td>\n",
                            "      <td>0.337884</td>\n",
                            "      <td>0.321832</td>\n",
                            "      <td>0.445212</td>\n",
                            "      <td>0.290258</td>\n",
                            "      <td>0.244476</td>\n",
                            "      <td>0.087914</td>\n",
                            "      <td>0.301831</td>\n",
                            "      <td>0.845702</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>5</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>C</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>C</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.285509</td>\n",
                            "      <td>0.860046</td>\n",
                            "      <td>0.798712</td>\n",
                            "      <td>0.835961</td>\n",
                            "      <td>0.391657</td>\n",
                            "      <td>0.288276</td>\n",
                            "      <td>0.549568</td>\n",
                            "      <td>0.905097</td>\n",
                            "      <td>0.850684</td>\n",
                            "      <td>0.693940</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>15</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>D</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.697272</td>\n",
                            "      <td>0.683600</td>\n",
                            "      <td>0.404089</td>\n",
                            "      <td>0.879379</td>\n",
                            "      <td>0.275549</td>\n",
                            "      <td>0.427871</td>\n",
                            "      <td>0.491667</td>\n",
                            "      <td>0.384315</td>\n",
                            "      <td>0.376689</td>\n",
                            "      <td>0.508099</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>16</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.719306</td>\n",
                            "      <td>0.777890</td>\n",
                            "      <td>0.730954</td>\n",
                            "      <td>0.644315</td>\n",
                            "      <td>1.024017</td>\n",
                            "      <td>0.391090</td>\n",
                            "      <td>0.988340</td>\n",
                            "      <td>0.411828</td>\n",
                            "      <td>0.393585</td>\n",
                            "      <td>0.461372</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>17</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>C</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>C</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.313032</td>\n",
                            "      <td>0.431007</td>\n",
                            "      <td>0.390992</td>\n",
                            "      <td>0.408874</td>\n",
                            "      <td>0.447887</td>\n",
                            "      <td>0.390253</td>\n",
                            "      <td>0.648932</td>\n",
                            "      <td>0.385935</td>\n",
                            "      <td>0.370401</td>\n",
                            "      <td>0.900412</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>19</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>E</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.489711</td>\n",
                            "      <td>0.533519</td>\n",
                            "      <td>0.451563</td>\n",
                            "      <td>0.513817</td>\n",
                            "      <td>0.429349</td>\n",
                            "      <td>0.489093</td>\n",
                            "      <td>0.326913</td>\n",
                            "      <td>0.440908</td>\n",
                            "      <td>0.694076</td>\n",
                            "      <td>0.444859</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>20</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>G</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.285503</td>\n",
                            "      <td>0.497190</td>\n",
                            "      <td>0.540631</td>\n",
                            "      <td>0.521383</td>\n",
                            "      <td>0.277155</td>\n",
                            "      <td>0.837442</td>\n",
                            "      <td>0.602099</td>\n",
                            "      <td>1.057592</td>\n",
                            "      <td>0.522523</td>\n",
                            "      <td>0.668214</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>21</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.346929</td>\n",
                            "      <td>0.774717</td>\n",
                            "      <td>0.374756</td>\n",
                            "      <td>0.838619</td>\n",
                            "      <td>0.641282</td>\n",
                            "      <td>0.854445</td>\n",
                            "      <td>0.834690</td>\n",
                            "      <td>0.347269</td>\n",
                            "      <td>0.741068</td>\n",
                            "      <td>0.257963</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>23</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>B</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>E</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.484546</td>\n",
                            "      <td>0.688881</td>\n",
                            "      <td>0.356810</td>\n",
                            "      <td>0.732911</td>\n",
                            "      <td>0.259619</td>\n",
                            "      <td>0.275222</td>\n",
                            "      <td>0.750423</td>\n",
                            "      <td>0.783911</td>\n",
                            "      <td>0.601736</td>\n",
                            "      <td>0.811545</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>29</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>A</td>\n",
                            "      <td>C</td>\n",
                            "      <td>B</td>\n",
                            "      <td>D</td>\n",
                            "      <td>A</td>\n",
                            "      <td>E</td>\n",
                            "      <td>A</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.425122</td>\n",
                            "      <td>0.758756</td>\n",
                            "      <td>0.706094</td>\n",
                            "      <td>0.871956</td>\n",
                            "      <td>0.584765</td>\n",
                            "      <td>0.550189</td>\n",
                            "      <td>0.775375</td>\n",
                            "      <td>0.798391</td>\n",
                            "      <td>0.972790</td>\n",
                            "      <td>0.283684</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>10 rows × 25 columns</p>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 84
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "source": [
                "#Getting Columns\n",
                "train_cols = list(data_train.columns)\n",
                "print(train_cols)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['id', 'cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'target']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "source": [
                "test_cols = list(data_test.columns)\n",
                "print(test_cols)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['id', 'cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "source": [
                "#Inspect for NaNs\n",
                "data_train.isnull().any()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "id        False\n",
                            "cat0      False\n",
                            "cat1      False\n",
                            "cat2      False\n",
                            "cat3      False\n",
                            "cat4      False\n",
                            "cat5      False\n",
                            "cat6      False\n",
                            "cat7      False\n",
                            "cat8      False\n",
                            "cat9      False\n",
                            "cont0     False\n",
                            "cont1     False\n",
                            "cont2     False\n",
                            "cont3     False\n",
                            "cont4     False\n",
                            "cont5     False\n",
                            "cont6     False\n",
                            "cont7     False\n",
                            "cont8     False\n",
                            "cont9     False\n",
                            "cont10    False\n",
                            "cont11    False\n",
                            "cont12    False\n",
                            "cont13    False\n",
                            "target    False\n",
                            "dtype: bool"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 86
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "source": [
                "data_test.isnull().any()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "id        False\n",
                            "cat0      False\n",
                            "cat1      False\n",
                            "cat2      False\n",
                            "cat3      False\n",
                            "cat4      False\n",
                            "cat5      False\n",
                            "cat6      False\n",
                            "cat7      False\n",
                            "cat8      False\n",
                            "cat9      False\n",
                            "cont0     False\n",
                            "cont1     False\n",
                            "cont2     False\n",
                            "cont3     False\n",
                            "cont4     False\n",
                            "cont5     False\n",
                            "cont6     False\n",
                            "cont7     False\n",
                            "cont8     False\n",
                            "cont9     False\n",
                            "cont10    False\n",
                            "cont11    False\n",
                            "cont12    False\n",
                            "cont13    False\n",
                            "dtype: bool"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 87
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Approach 1a: Drop all categorical columns"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "source": [
                "#Code snippet\n",
                "#dropna data=data.dropna(axis=0) which i think means rows\n",
                "\n",
                "#Gain y from train data\n",
                "y_train_1a = data_train.target.copy()\n",
                "\n",
                "features_nocat = [col for col in train_cols if 'cat' not in col]\n",
                "print(features_nocat)\n",
                "#Gain X from train data\n",
                "\n",
                "X_train_1a = data_train.drop('target', axis=1).copy()\n",
                "\n",
                "#alternatively\n",
                "#train_cols.pop('target') => maybe this is a thing? Idk haven't checked\n",
                "# X_train = data_train[train_cols]"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['id', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'target']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz1\n",
                "#### Specify Model\n",
                "iowa_model = DecisionTreeRegressor()\n",
                "#### Fit Model\n",
                "iowa_model.fit(X, y)\n",
                "\n",
                "print(\"First in-sample predictions:\", iowa_model.predict(X.head()))\n",
                "print(\"Actual target values for those homes:\", y.head().tolist())\n",
                "\n",
                "\n",
                "#### Import the train_test_split function and uncomment\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#### fill in and uncomment\n",
                "train_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)\n",
                "\n",
                "\n",
                "\n",
                "#### Specify the model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "\n",
                "#### Fit iowa_model with the training data.\n",
                "iowa_model.fit(train_X,train_y)\n",
                "\n",
                "\n",
                "#### Predict with all validation observations\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "\n",
                "\n",
                "#### print the top few validation predictions\n",
                "print(val_predictions[:5])\n",
                "#### print the top few actual prices from validation data\n",
                "print(val_y[:5])\n",
                "\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "val_mae = mean_absolute_error(val_y, val_predictions)\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz2\n",
                "#### Read the data\n",
                "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
                "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "#### Obtain target and predictors\n",
                "y = X_full.SalePrice\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = X_full[features].copy()\n",
                "X_test = X_test_full[features].copy()\n",
                "\n",
                "#### Break off validation set from training data\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "#### Define the models\n",
                "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
                "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
                "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
                "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
                "\n",
                "models = [model_1, model_2, model_3, model_4, model_5]\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "#### Function for comparing different models\n",
                "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
                "    model.fit(X_t, y_t)\n",
                "    preds = model.predict(X_v)\n",
                "    return mean_absolute_error(y_v, preds)\n",
                "\n",
                "for i in range(0, len(models)):\n",
                "    mae = score_model(models[i])\n",
                "    print(\"Model %d MAE: %d\" % (i+1, mae))\n",
                "\n",
                "#### Fill in the best model\n",
                "best_model = model_3\n",
                "\n",
                "#### Define a model\n",
                "my_model = best_model \n",
                "#### Define a model\n",
                "my_model = best_model\n",
                "\n",
                "#### Fit the model to the training data\n",
                "my_model.fit(X, y)\n",
                "\n",
                "#### Generate test predictions\n",
                "preds_test = my_model.predict(X_test)\n",
                "\n",
                "#### Save predictions in format used for competition scoring\n",
                "output = pd.DataFrame({'Id': X_test.index,\n",
                "                       'SalePrice': preds_test})\n",
                "output.to_csv('submission.csv', index=False)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz3\n",
                "#### Import helpful libraries\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#### Load the data, and separate the target\n",
                "iowa_file_path = '../input/train.csv'\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "y = home_data.SalePrice\n",
                "\n",
                "#### Create X (After completing the exercise, you can return to modify this line!)\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "\n",
                "#### Select columns corresponding to features, and preview the data\n",
                "X = home_data[features]\n",
                "X.head()\n",
                "\n",
                "#### Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "#### Define a random forest model\n",
                "rf_model = RandomForestRegressor(random_state=1)\n",
                "rf_model.fit(train_X, train_y)\n",
                "rf_val_predictions = rf_model.predict(val_X)\n",
                "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
                "\n",
                "print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n",
                "\n",
                "#### To improve accuracy, create a new Random Forest model which you will train on all training data\n",
                "rf_model_on_full_data = RandomForestRegressor(random_state=2)\n",
                "\n",
                "\n",
                "#### fit rf_model_on_full_data on all data from the training data\n",
                "rf_model_on_full_data.fit(X,y)\n",
                "\n",
                "#### path to file you will use for predictions\n",
                "test_data_path = '../input/test.csv'\n",
                "\n",
                "#### read test data file using pandas\n",
                "test_data = pd.read_csv(test_data_path)\n",
                "\n",
                "#### create test_X which comes from test_data but includes only the columns you used for prediction.\n",
                "#### The list of columns is stored in a variable called features\n",
                "test_X = test_data[features]\n",
                "\n",
                "#### make predictions which we will submit. \n",
                "test_preds = rf_model_on_full_data.predict(test_X)\n",
                "test_preds\n",
                "\n",
                "output = pd.DataFrame({'Id': test_data.Id,\n",
                "                       'SalePrice': test_preds})\n",
                "output.to_csv('submission.csv', index=False)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz4\n",
                "#### Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "\n",
                "#### Path of the file to read\n",
                "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "#### Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "#### Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "#### Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "#### Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "#### Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "#### Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "#### Using best value for max_leaf_nodes\n",
                "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
                "iowa_model.fit(train_X, train_y)\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "#### Define the model. Set random_state to 1\n",
                "rf_model = RandomForestRegressor(random_state=1)\n",
                "\n",
                "#### fit your model\n",
                "rf_model.fit(train_X,train_y)\n",
                "\n",
                "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
                "rf_val_mae = mean_absolute_error(val_y,rf_model.predict(val_X))\n",
                "\n",
                "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n",
                "\n",
                "\n",
                "rf_model = RandomForestRegressor()\n",
                "\n",
                "#### fit your model\n",
                "rf_model.fit(train_X, train_y)\n",
                "\n",
                "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
                "rf_val_predictions = rf_model.predict(val_X)\n",
                "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz5\n",
                "#### Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "\n",
                "#### Path of the file to read\n",
                "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "#### Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "#### Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "#### Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "#### Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "#### Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "#### Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "#### Using best value for max_leaf_nodes\n",
                "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
                "iowa_model.fit(train_X, train_y)\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "#### Define the model. Set random_state to 1\n",
                "rf_model = RandomForestRegressor(random_state=1)\n",
                "\n",
                "#### fit your model\n",
                "rf_model.fit(train_X,train_y)\n",
                "\n",
                "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
                "rf_val_mae = mean_absolute_error(val_y,rf_model.predict(val_X))\n",
                "\n",
                "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n",
                "\n",
                "\n",
                "rf_model = RandomForestRegressor()\n",
                "\n",
                "#### fit your model\n",
                "rf_model.fit(train_X, train_y)\n",
                "\n",
                "#### Calculate the mean absolute error of your Random Forest model on the validation data\n",
                "rf_val_predictions = rf_model.predict(val_X)\n",
                "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Ansatz6 \n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#### Read the data\n",
                "X = pd.read_csv('../input/train.csv', index_col='Id')\n",
                "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "#### Remove rows with missing target, separate target from predictors\n",
                "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
                "y = X.SalePrice              \n",
                "X.drop(['SalePrice'], axis=1, inplace=True)\n",
                "\n",
                "#### Break off validation set from training data\n",
                "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
                "                                                                random_state=0)\n",
                "\n",
                "#### \"Cardinality\" means the number of unique values in a column\n",
                "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
                "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
                "                        X_train_full[cname].dtype == \"object\"]\n",
                "\n",
                "#### Select numeric columns\n",
                "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
                "\n",
                "#### Keep selected columns only\n",
                "my_cols = low_cardinality_cols + numeric_cols\n",
                "X_train = X_train_full[my_cols].copy()\n",
                "X_valid = X_valid_full[my_cols].copy()\n",
                "X_test = X_test_full[my_cols].copy()\n",
                "\n",
                "#### One-hot encode the data (to shorten the code, we use pandas)\n",
                "X_train = pd.get_dummies(X_train)\n",
                "X_valid = pd.get_dummies(X_valid)\n",
                "X_test = pd.get_dummies(X_test)\n",
                "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
                "X_train, X_test = X_train.align(X_test, join='left', axis=1)\n",
                "\n",
                "from xgboost import XGBRegressor\n",
                "\n",
                "#### Define the model\n",
                "my_model_1 = XGBRegressor(random_state=0) # Your code here\n",
                "\n",
                "#### Fit the model\n",
                "my_model_1.fit(X_train,y_train) # Your code here\n",
                "\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "#### Get predictions\n",
                "predictions_1 = my_model_1.predict(X_valid) # Your code here\n",
                "\n",
                "\n",
                "#### Calculate MAE\n",
                "mae_1 = mean_absolute_error(y_valid, predictions_1) # Your code here\n",
                "\n",
                "#### Uncomment to print MAE\n",
                "print(\"Mean Absolute Error:\" , mae_1)\n",
                " \n",
                "#### Define the model\n",
                "my_model_2 = XGBRegressor(n_estimators=1000,learning_rate=0.02, random_state=1) # Your code here\n",
                "\n",
                "#### Fit the model\n",
                "my_model_2.fit(X_train,y_train) # Your code here\n",
                "\n",
                "#### Get predictions\n",
                "predictions_2 = my_model_2.predict(X_valid) # Your code here\n",
                "\n",
                "#### Calculate MAE\n",
                "mae_2 = mean_absolute_error(y_valid, predictions_2) # Your code here\n",
                "\n",
                "#### Uncomment to print MAE\n",
                "print(\"Mean Absolute Error:\" , mae_2)\n",
                "mae_2<mae_1\n",
                "\n",
                "\n",
                "#### Step 3: Break the model\n",
                "\n",
                "#### Define the model\n",
                "my_model_3 = XGBRegressor(n_estimators=10000, early_stopping_rounds=20, random_state=3)\n",
                "\n",
                "#### Fit the model\n",
                "my_model_3.fit(X_train,y_train)# Your code here\n",
                "\n",
                "#### Get predictions\n",
                "predictions_3 = my_model_3.predict(X_valid)\n",
                "\n",
                "#### Calculate MAE\n",
                "mae_3 = mean_absolute_error(y_valid, predictions_3)\n",
                "\n",
                "#### Uncomment to print MAE\n",
                "print(\"Mean Absolute Error:\" , mae_3)\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "source": [
                "## Ansatz7\n",
                "\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#### Read the data\n",
                "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
                "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "#### Remove rows with missing target, separate target from predictors\n",
                "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
                "y = X_full.SalePrice\n",
                "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
                "\n",
                "#### Break off validation set from training data\n",
                "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
                "                                                                train_size=0.8, test_size=0.2,\n",
                "                                                                random_state=0)\n",
                "\n",
                "#### \"Cardinality\" means the number of unique values in a column\n",
                "#### Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
                "categorical_cols = [cname for cname in X_train_full.columns if\n",
                "                    X_train_full[cname].nunique() < 10 and \n",
                "                    X_train_full[cname].dtype == \"object\"]\n",
                "\n",
                "#### Select numerical columns\n",
                "numerical_cols = [cname for cname in X_train_full.columns if \n",
                "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
                "\n",
                "#### Keep selected columns only\n",
                "my_cols = categorical_cols + numerical_cols\n",
                "X_train = X_train_full[my_cols].copy()\n",
                "X_valid = X_valid_full[my_cols].copy()\n",
                "X_test = X_test_full[my_cols].copy()\n",
                "\n",
                "\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "#### Preprocessing for numerical data\n",
                "numerical_transformer = SimpleImputer(strategy='constant')\n",
                "\n",
                "#### Preprocessing for categorical data\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "#### Bundle preprocessing for numerical and categorical data\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_cols),\n",
                "        ('cat', categorical_transformer, categorical_cols)\n",
                "    ])\n",
                "\n",
                "#### Define model\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "\n",
                "#### Bundle preprocessing and modeling code in a pipeline\n",
                "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                      ('model', model)\n",
                "                     ])\n",
                "\n",
                "#### Preprocessing of training data, fit model \n",
                "clf.fit(X_train, y_train)\n",
                "\n",
                "#### Preprocessing of validation data, get predictions\n",
                "preds = clf.predict(X_valid)\n",
                "\n",
                "print('MAE:', mean_absolute_error(y_valid, preds))\n",
                "MAE: 17861.780102739725\n",
                "The code yields a value around 17862 for the mean absolute error (MAE). In the next step, you will amend the code to do better.\n",
                "\n",
                "Step 1: Improve the performance\n",
                "Part A\n",
                "\n",
                "Now, it's your turn! In the code cell below, define your own preprocessing steps and random forest model. Fill in values for the following variables:\n",
                "\n",
                "numerical_transformer\n",
                "categorical_transformer\n",
                "model\n",
                "To pass this part of the exercise, you need only define valid preprocessing steps and a random forest model.\n",
                "\n",
                "In [5]:\n",
                "# Preprocessing for numerical data\n",
                "numerical_transformer = SimpleImputer(strategy='constant') # Your code here\n",
                "\n",
                "# Preprocessing for categorical data\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "    \n",
                "]) \n",
                "# Your code here\n",
                "\n",
                "# Bundle preprocessing for numerical and categorical data\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_cols),\n",
                "        ('cat', categorical_transformer, categorical_cols)\n",
                "    ])\n",
                "\n",
                "# Define model\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=20) # Your code here\n",
                "\n",
                "# Check your answer\n",
                "step_1.a.check()\n",
                "Correct\n",
                "\n",
                "In [6]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_1.a.hint()\n",
                "step_1.a.solution()\n",
                "Solution:\n",
                "\n",
                "# Preprocessing for numerical data\n",
                "numerical_transformer = SimpleImputer(strategy='constant')\n",
                "\n",
                "# Preprocessing for categorical data\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "# Bundle preprocessing for numerical and categorical data\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_cols),\n",
                "        ('cat', categorical_transformer, categorical_cols)\n",
                "    ])\n",
                "\n",
                "# Define model\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "Part B\n",
                "\n",
                "Run the code cell below without changes.\n",
                "\n",
                "To pass this step, you need to have defined a pipeline in Part A that achieves lower MAE than the code above. You're encouraged to take your time here and try out many different approaches, to see how low you can get the MAE! (If your code does not pass, please amend the preprocessing steps and model in Part A.)\n",
                "\n",
                "In [7]:\n",
                "# Bundle preprocessing and modeling code in a pipeline\n",
                "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                              ('model', model)\n",
                "                             ])\n",
                "\n",
                "# Preprocessing of training data, fit model \n",
                "my_pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Preprocessing of validation data, get predictions\n",
                "preds = my_pipeline.predict(X_valid)\n",
                "\n",
                "# Evaluate the model\n",
                "score = mean_absolute_error(y_valid, preds)\n",
                "print('MAE:', score)\n",
                "\n",
                "# Check your answer\n",
                "step_1.b.check()\n",
                "MAE: 17287.355445205478\n",
                "Correct\n",
                "\n",
                "In [8]:\n",
                "# Line below will give you a hint\n",
                "step_1.b.hint()\n",
                "Hint: Please see the hint from Part A to get some ideas for how to change the preprocessing steps and model to get better performance.\n",
                "\n",
                "Step 2: Generate test predictions\n",
                "\n",
                "\n",
                "# Preprocessing of test data, fit model\n",
                "preds_test = my_pipeline.predict(X_test) # Your code here\n",
                "\n",
                "\n",
                "\n",
                "# Preprocessing of test data, fit model\n",
                "preds_test = my_pipeline.predict(X_test)\n",
                "Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition.\n",
                "\n",
                "# Save test predictions to file\n",
                "output = pd.DataFrame({'Id': X_test.index,\n",
                "                       'SalePrice': preds_test})\n",
                "output.to_csv('submission.csv', index=False)"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-66-f29c935557cf>, line 1)",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-f29c935557cf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    This notebook is an exercise in the Intermediate Machine Learning course. You can reference the tutorial at this link.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "source": [
                "\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Read the data\n",
                "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
                "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "# Remove rows with missing target, separate target from predictors\n",
                "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
                "y = X_full.SalePrice\n",
                "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
                "\n",
                "# To keep things simple, we'll use only numerical predictors\n",
                "X = X_full.select_dtypes(exclude=['object'])\n",
                "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
                "\n",
                "# Break off validation set from training data\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "Use the next code cell to print the first five rows of the data.\n",
                "\n",
                "In [3]:\n",
                "X_train.head()\n",
                "Out[3]:\n",
                "MSSubClass\tLotFrontage\tLotArea\tOverallQual\tOverallCond\tYearBuilt\tYearRemodAdd\tMasVnrArea\tBsmtFinSF1\tBsmtFinSF2\t...\tGarageArea\tWoodDeckSF\tOpenPorchSF\tEnclosedPorch\t3SsnPorch\tScreenPorch\tPoolArea\tMiscVal\tMoSold\tYrSold\n",
                "Id\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
                "619\t20\t90.0\t11694\t9\t5\t2007\t2007\t452.0\t48\t0\t...\t774\t0\t108\t0\t0\t260\t0\t0\t7\t2007\n",
                "871\t20\t60.0\t6600\t5\t5\t1962\t1962\t0.0\t0\t0\t...\t308\t0\t0\t0\t0\t0\t0\t0\t8\t2009\n",
                "93\t30\t80.0\t13360\t5\t7\t1921\t2006\t0.0\t713\t0\t...\t432\t0\t0\t44\t0\t0\t0\t0\t8\t2009\n",
                "818\t20\tNaN\t13265\t8\t5\t2002\t2002\t148.0\t1218\t0\t...\t857\t150\t59\t0\t0\t0\t0\t0\t7\t2008\n",
                "303\t20\t118.0\t13704\t7\t5\t2001\t2002\t150.0\t0\t0\t...\t843\t468\t81\t0\t0\t0\t0\t0\t1\t2006\n",
                "5 rows × 36 columns\n",
                "\n",
                "You can already see a few missing values in the first several rows. In the next step, you'll obtain a more comprehensive understanding of the missing values in the dataset.\n",
                "\n",
                "Step 1: Preliminary investigation\n",
                "Run the code cell below without changes.\n",
                "\n",
                "In [4]:\n",
                "# Shape of training data (num_rows, num_columns)\n",
                "print(X_train.shape)\n",
                "\n",
                "# Number of missing values in each column of training data\n",
                "missing_val_count_by_column = (X_train.isnull().sum())\n",
                "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
                "(1168, 36)\n",
                "LotFrontage    212\n",
                "MasVnrArea       6\n",
                "GarageYrBlt     58\n",
                "dtype: int64\n",
                "In [5]:\n",
                "X_train.isnull().sum()[X_train.isnull().sum()>0].iloc[:0]\n",
                "Out[5]:\n",
                "Series([], dtype: int64)\n",
                "Part A\n",
                "\n",
                "Use the above output to answer the questions below.\n",
                "\n",
                "In [6]:\n",
                "# Fill in the line below: How many rows are in the training data?\n",
                "num_rows = 1168\n",
                "\n",
                "# Fill in the line below: How many columns in the training data\n",
                "# have missing values?\n",
                "num_cols_with_missing = 3\n",
                "\n",
                "# Fill in the line below: How many missing entries are contained in \n",
                "# all of the training data?\n",
                "tot_missing = 276\n",
                "\n",
                "# Check your answers\n",
                "step_1.a.check()\n",
                "Correct\n",
                "\n",
                "In [7]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_1.a.hint()\n",
                "#step_1.a.solution()\n",
                "Part B\n",
                "\n",
                "Considering your answers above, what do you think is likely the best approach to dealing with the missing values?\n",
                "\n",
                "In [8]:\n",
                "# Check your answer (Run this code cell to receive credit!)\n",
                "step_1.b.check()\n",
                "Correct:\n",
                "\n",
                "Since there are relatively few missing entries in the data (the column with the greatest percentage of missing values is missing less than 20% of its entries), we can expect that dropping columns is unlikely to yield good results. This is because we'd be throwing away a lot of valuable data, and so imputation will likely perform better.\n",
                "\n",
                "In [9]:\n",
                "step_1.b.hint()\n",
                "Hint: Does the dataset have a lot of missing values, or just a few? Would we lose much information if we completely ignored the columns with missing entries?\n",
                "\n",
                "To compare different approaches to dealing with missing values, you'll use the same score_dataset() function from the tutorial. This function reports the mean absolute error (MAE) from a random forest model.\n",
                "\n",
                "In [10]:\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "# Function for comparing different approaches\n",
                "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
                "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "    model.fit(X_train, y_train)\n",
                "    preds = model.predict(X_valid)\n",
                "    return mean_absolute_error(y_valid, preds)\n",
                "Step 2: Drop columns with missing values\n",
                "In this step, you'll preprocess the data in X_train and X_valid to remove columns with missing values. Set the preprocessed DataFrames to reduced_X_train and reduced_X_valid, respectively.\n",
                "\n",
                "In [11]:\n",
                "# Fill in the line below: get names of columns with missing values\n",
                "miss_col = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
                "miss_col\n",
                "\n",
                "# Fill in the lines below: drop columns in training and validation data\n",
                "reduced_X_train = X_train.drop(miss_col, axis=1) # 1 axis as in cols\n",
                "reduced_X_valid = X_valid.drop(miss_col, axis=1)\n",
                "\n",
                "# Check your answers\n",
                "step_2.check()\n",
                "Correct\n",
                "\n",
                "In [12]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_2.hint()\n",
                "#step_2.solution()\n",
                "Run the next code cell without changes to obtain the MAE for this approach.\n",
                "\n",
                "In [13]:\n",
                "print(\"MAE (Drop columns with missing values):\")\n",
                "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
                "MAE (Drop columns with missing values):\n",
                "17837.82570776256\n",
                "Step 3: Imputation\n",
                "Part A\n",
                "\n",
                "Use the next code cell to impute missing values with the mean value along each column. Set the preprocessed DataFrames to imputed_X_train and imputed_X_valid. Make sure that the column names match those in X_train and X_valid.\n",
                "\n",
                "In [14]:\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "# Fill in the lines below: imputation\n",
                "____ # Your code here\n",
                "impute=SimpleImputer()\n",
                "imputed_X_train = pd.DataFrame(impute.fit_transform(X_train))\n",
                "imputed_X_valid = pd.DataFrame(impute.transform(X_valid))\n",
                "\n",
                "# Fill in the lines below: imputation removed column names; put them back\n",
                "imputed_X_train.columns = X_train.columns\n",
                "imputed_X_valid.columns = X_valid.columns\n",
                "\n",
                "# Check your answers\n",
                "step_3.a.check()\n",
                "Correct\n",
                "\n",
                "In [15]:\n",
                "# Lines below will give you a hint or solution code\n",
                "step_3.a.hint()\n",
                "step_3.a.solution()\n",
                "Hint: Begin by defining an instance of the SimpleImputer() class. Then, use the imputer to fit and transform the training data, before transforming the validation data. Get the original column names from the original DataFrames X_train and X_valid.\n",
                "\n",
                "Solution:\n",
                "\n",
                "# Imputation\n",
                "my_imputer = SimpleImputer()\n",
                "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
                "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
                "\n",
                "# Imputation removed column names; put them back\n",
                "imputed_X_train.columns = X_train.columns\n",
                "imputed_X_valid.columns = X_valid.columns\n",
                "Run the next code cell without changes to obtain the MAE for this approach.\n",
                "\n",
                "In [16]:\n",
                "print(\"MAE (Imputation):\")\n",
                "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
                "MAE (Imputation):\n",
                "18062.894611872147\n",
                "Part B\n",
                "\n",
                "Compare the MAE from each approach. Does anything surprise you about the results? Why do you think one approach performed better than the other?\n",
                "\n",
                "In [17]:\n",
                "# Check your answer (Run this code cell to receive credit!)\n",
                "step_3.b.check()\n",
                "Correct:\n",
                "\n",
                "Given that thre are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. For instance, consider the GarageYrBlt column (which indicates the year that the garage was built). It's likely that in some cases, a missing value could indicate a house that does not have a garage. Does it make more sense to fill in the median value along each column in this case? Or could we get better results by filling in the minimum value along each column? It's not quite clear what's best in this case, but perhaps we can rule out some options immediately - for instance, setting missing values in this column to 0 is likely to yield horrible results!\n",
                "\n",
                "In [18]:\n",
                "#step_3.b.hint()\n",
                "Step 4: Generate test predictions\n",
                "In this final step, you'll use any approach of your choosing to deal with missing values. Once you've preprocessed the training and validation features, you'll train and evaluate a random forest model. Then, you'll preprocess the test data before generating predictions that can be submitted to the competition!\n",
                "\n",
                "Part A\n",
                "\n",
                "Use the next code cell to preprocess the training and validation data. Set the preprocessed DataFrames to final_X_train and final_X_valid. You can use any approach of your choosing here! in order for this step to be marked as correct, you need only ensure:\n",
                "\n",
                "the preprocessed DataFrames have the same number of columns,\n",
                "the preprocessed DataFrames have no missing values,\n",
                "final_X_train and y_train have the same number of rows, and\n",
                "final_X_valid and y_valid have the same number of rows.\n",
                "In [19]:\n",
                "# Preprocessed training and validation features\n",
                "final_X_train = reduced_X_train\n",
                "final_X_valid = reduced_X_valid\n",
                "\n",
                "# Check your answers\n",
                "step_4.a.check()\n",
                "Correct\n",
                "\n",
                "In [20]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_4.a.hint()\n",
                "#step_4.a.solution()\n",
                "Run the next code cell to train and evaluate a random forest model. (Note that we don't use the score_dataset() function above, because we will soon use the trained model to generate test predictions!)\n",
                "\n",
                "In [21]:\n",
                "# Define and fit model\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "model.fit(final_X_train, y_train)\n",
                "\n",
                "# Get validation predictions and MAE\n",
                "preds_valid = model.predict(final_X_valid)\n",
                "print(\"MAE (Your approach):\")\n",
                "print(mean_absolute_error(y_valid, preds_valid))\n",
                "MAE (Your approach):\n",
                "17837.82570776256\n",
                "Part B\n",
                "\n",
                "Use the next code cell to preprocess your test data. Make sure that you use a method that agrees with how you preprocessed the training and validation data, and set the preprocessed test features to final_X_test.\n",
                "\n",
                "Then, use the preprocessed test features and the trained model to generate test predictions in preds_test.\n",
                "\n",
                "In order for this step to be marked correct, you need only ensure:\n",
                "\n",
                "the preprocessed test DataFrame has no missing values, and\n",
                "final_X_test has the same number of rows as X_test.\n",
                "In [22]:\n",
                "# Fill in the line below: preprocess test data\n",
                "impute_X_test= pd.DataFrame(impute.transform(X_test))\n",
                "impute_X_test.columns = X_test.columns\n",
                "\n",
                "final_X_test = impute_X_test.drop(miss_col, axis=1)\n",
                "\n",
                "# Fill in the line below: get test predictions\n",
                "preds_test = model.predict(final_X_test)\n",
                "\n",
                "# Check your answers\n",
                "step_4.b.check()\n",
                "Correct\n",
                "\n",
                "In [23]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_4.b.hint()\n",
                "step_4.b.solution()\n",
                "Solution:\n",
                "\n",
                "# Preprocess test data\n",
                "final_X_test = pd.DataFrame(final_imputer.transform(X_test))\n",
                "\n",
                "# Get test predictions\n",
                "preds_test = model.predict(final_X_test)\n",
                "Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition.\n",
                "\n",
                "In [24]:\n",
                "# Save test predictions to file\n",
                "output = pd.DataFrame({'Id': X_test.index,\n",
                "                       'SalePrice': preds_test})\n",
                "output.to_csv('submission.csv', index=False)\n",
                "X_test.index\n",
                "Out[24]:\n",
                "Int64Index([1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470,\n",
                "            ...\n",
                "            2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919],\n",
                "           dtype='int64', name='Id', length=1459)\n",
                "Submit your results\n",
                "Once you have successfully completed Step 4, you're ready to submit your results to the leaderboard! (You also learned how to do this in the previous exercise. If you need a reminder of how to do this, please use the instructions below.)\n",
                "\n",
                "First, you'll need to join the competition if you haven't already. So open a new window by clicking on this link. Then click on the Join Competition button.\n",
                "\n",
                "join competition image\n",
                "\n",
                "Next, follow the instructions below:\n",
                "\n",
                "Begin by clicking on the blue Save Version button in the top right corner of the window. This will generate a pop-up window.\n",
                "Ensure that the Save and Run All option is selected, and then click on the blue Save button.\n",
                "This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
                "Click on the Output tab on the right of the screen. Then, click on the file you would like to submit, and click on the blue Submit button to submit your results to the leaderboard.\n",
                "You have now successfully submitted to the competition!\n",
                "\n",
                "If you want to keep working to improve your performance, select the blue Edit button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n",
                "\n",
                "Keep going\n",
                "Move on to learn what categorical variables are, along with how to incorporate them into your machine learning models. Categorical variables are very common in real-world data, but you'll get an error if you try to plug them into your models without processing them first!\n",
                "\n",
                "Have questions or comments? Visit the Learn Discussion forum to chat with other Learners."
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-67-3060d175032b>, line 1)",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-3060d175032b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    This notebook is an exercise in the Intermediate Machine Learning course. You can reference the tutorial at this link.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Read the data\n",
                "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
                "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "# Remove rows with missing target, separate target from predictors\n",
                "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
                "y = X_full.SalePrice\n",
                "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
                "\n",
                "# To keep things simple, we'll use only numerical predictors\n",
                "X = X_full.select_dtypes(exclude=['object'])\n",
                "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
                "\n",
                "# Break off validation set from training data\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# Shape of training data (num_rows, num_columns)\n",
                "print(X_train.shape)\n",
                "\n",
                "# Number of missing values in each column of training data\n",
                "missing_val_count_by_column = (X_train.isnull().sum())\n",
                "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
                "(1168, 36)\n",
                "LotFrontage    212\n",
                "MasVnrArea       6\n",
                "GarageYrBlt     58\n",
                "dtype: int64\n",
                "In [5]:\n",
                "X_train.isnull().sum()[X_train.isnull().sum()>0].iloc[:0]\n",
                "Out[5]:\n",
                "Series([], dtype: int64)\n",
                "Part A\n",
                "\n",
                "Use the above output to answer the questions below.\n",
                "\n",
                "In [6]:\n",
                "# Fill in the line below: How many rows are in the training data?\n",
                "num_rows = 1168\n",
                "\n",
                "# Fill in the line below: How many columns in the training data\n",
                "# have missing values?\n",
                "num_cols_with_missing = 3\n",
                "\n",
                "# Fill in the line below: How many missing entries are contained in \n",
                "# all of the training data?\n",
                "tot_missing = 276\n",
                "\n",
                "# Check your answers\n",
                "step_1.a.check()\n",
                "Correct\n",
                "\n",
                "In [7]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_1.a.hint()\n",
                "#step_1.a.solution()\n",
                "Part B\n",
                "\n",
                "Considering your answers above, what do you think is likely the best approach to dealing with the missing values?\n",
                "\n",
                "In [8]:\n",
                "# Check your answer (Run this code cell to receive credit!)\n",
                "step_1.b.check()\n",
                "Correct:\n",
                "\n",
                "Since there are relatively few missing entries in the data (the column with the greatest percentage of missing values is missing less than 20% of its entries), we can expect that dropping columns is unlikely to yield good results. This is because we'd be throwing away a lot of valuable data, and so imputation will likely perform better.\n",
                "\n",
                "In [9]:\n",
                "step_1.b.hint()\n",
                "Hint: Does the dataset have a lot of missing values, or just a few? Would we lose much information if we completely ignored the columns with missing entries?\n",
                "\n",
                "To compare different approaches to dealing with missing values, you'll use the same score_dataset() function from the tutorial. This function reports the mean absolute error (MAE) from a random forest model.\n",
                "\n",
                "In [10]:\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "# Function for comparing different approaches\n",
                "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
                "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "    model.fit(X_train, y_train)\n",
                "    preds = model.predict(X_valid)\n",
                "    return mean_absolute_error(y_valid, preds)\n",
                "Step 2: Drop columns with missing values\n",
                "In this step, you'll preprocess the data in X_train and X_valid to remove columns with missing values. Set the preprocessed DataFrames to reduced_X_train and reduced_X_valid, respectively.\n",
                "\n",
                "In [11]:\n",
                "# Fill in the line below: get names of columns with missing values\n",
                "miss_col = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
                "miss_col\n",
                "\n",
                "# Fill in the lines below: drop columns in training and validation data\n",
                "reduced_X_train = X_train.drop(miss_col, axis=1) # 1 axis as in cols\n",
                "reduced_X_valid = X_valid.drop(miss_col, axis=1)\n",
                "\n",
                "# Check your answers\n",
                "step_2.check()\n",
                "Correct\n",
                "\n",
                "In [12]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_2.hint()\n",
                "#step_2.solution()\n",
                "Run the next code cell without changes to obtain the MAE for this approach.\n",
                "\n",
                "In [13]:\n",
                "print(\"MAE (Drop columns with missing values):\")\n",
                "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
                "MAE (Drop columns with missing values):\n",
                "17837.82570776256\n",
                "Step 3: Imputation\n",
                "Part A\n",
                "\n",
                "Use the next code cell to impute missing values with the mean value along each column. Set the preprocessed DataFrames to imputed_X_train and imputed_X_valid. Make sure that the column names match those in X_train and X_valid.\n",
                "\n",
                "In [14]:\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "# Fill in the lines below: imputation\n",
                "____ # Your code here\n",
                "impute=SimpleImputer()\n",
                "imputed_X_train = pd.DataFrame(impute.fit_transform(X_train))\n",
                "imputed_X_valid = pd.DataFrame(impute.transform(X_valid))\n",
                "\n",
                "# Fill in the lines below: imputation removed column names; put them back\n",
                "imputed_X_train.columns = X_train.columns\n",
                "imputed_X_valid.columns = X_valid.columns\n",
                "\n",
                "# Check your answers\n",
                "step_3.a.check()\n",
                "Correct\n",
                "\n",
                "In [15]:\n",
                "# Lines below will give you a hint or solution code\n",
                "step_3.a.hint()\n",
                "step_3.a.solution()\n",
                "Hint: Begin by defining an instance of the SimpleImputer() class. Then, use the imputer to fit and transform the training data, before transforming the validation data. Get the original column names from the original DataFrames X_train and X_valid.\n",
                "\n",
                "Solution:\n",
                "\n",
                "# Imputation\n",
                "my_imputer = SimpleImputer()\n",
                "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
                "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
                "\n",
                "# Imputation removed column names; put them back\n",
                "imputed_X_train.columns = X_train.columns\n",
                "imputed_X_valid.columns = X_valid.columns\n",
                "Run the next code cell without changes to obtain the MAE for this approach.\n",
                "\n",
                "In [16]:\n",
                "print(\"MAE (Imputation):\")\n",
                "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
                "MAE (Imputation):\n",
                "18062.894611872147\n",
                "Part B\n",
                "\n",
                "Compare the MAE from each approach. Does anything surprise you about the results? Why do you think one approach performed better than the other?\n",
                "\n",
                "In [17]:\n",
                "# Check your answer (Run this code cell to receive credit!)\n",
                "step_3.b.check()\n",
                "Correct:\n",
                "\n",
                "Given that thre are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. For instance, consider the GarageYrBlt column (which indicates the year that the garage was built). It's likely that in some cases, a missing value could indicate a house that does not have a garage. Does it make more sense to fill in the median value along each column in this case? Or could we get better results by filling in the minimum value along each column? It's not quite clear what's best in this case, but perhaps we can rule out some options immediately - for instance, setting missing values in this column to 0 is likely to yield horrible results!\n",
                "\n",
                "In [18]:\n",
                "#step_3.b.hint()\n",
                "Step 4: Generate test predictions\n",
                "In this final step, you'll use any approach of your choosing to deal with missing values. Once you've preprocessed the training and validation features, you'll train and evaluate a random forest model. Then, you'll preprocess the test data before generating predictions that can be submitted to the competition!\n",
                "\n",
                "Part A\n",
                "\n",
                "Use the next code cell to preprocess the training and validation data. Set the preprocessed DataFrames to final_X_train and final_X_valid. You can use any approach of your choosing here! in order for this step to be marked as correct, you need only ensure:\n",
                "\n",
                "the preprocessed DataFrames have the same number of columns,\n",
                "the preprocessed DataFrames have no missing values,\n",
                "final_X_train and y_train have the same number of rows, and\n",
                "final_X_valid and y_valid have the same number of rows.\n",
                "In [19]:\n",
                "# Preprocessed training and validation features\n",
                "final_X_train = reduced_X_train\n",
                "final_X_valid = reduced_X_valid\n",
                "\n",
                "# Check your answers\n",
                "step_4.a.check()\n",
                "Correct\n",
                "\n",
                "In [20]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_4.a.hint()\n",
                "#step_4.a.solution()\n",
                "Run the next code cell to train and evaluate a random forest model. (Note that we don't use the score_dataset() function above, because we will soon use the trained model to generate test predictions!)\n",
                "\n",
                "In [21]:\n",
                "# Define and fit model\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "model.fit(final_X_train, y_train)\n",
                "\n",
                "# Get validation predictions and MAE\n",
                "preds_valid = model.predict(final_X_valid)\n",
                "print(\"MAE (Your approach):\")\n",
                "print(mean_absolute_error(y_valid, preds_valid))\n",
                "MAE (Your approach):\n",
                "17837.82570776256\n",
                "Part B\n",
                "\n",
                "Use the next code cell to preprocess your test data. Make sure that you use a method that agrees with how you preprocessed the training and validation data, and set the preprocessed test features to final_X_test.\n",
                "\n",
                "Then, use the preprocessed test features and the trained model to generate test predictions in preds_test.\n",
                "\n",
                "In order for this step to be marked correct, you need only ensure:\n",
                "\n",
                "the preprocessed test DataFrame has no missing values, and\n",
                "final_X_test has the same number of rows as X_test.\n",
                "In [22]:\n",
                "# Fill in the line below: preprocess test data\n",
                "impute_X_test= pd.DataFrame(impute.transform(X_test))\n",
                "impute_X_test.columns = X_test.columns\n",
                "\n",
                "final_X_test = impute_X_test.drop(miss_col, axis=1)\n",
                "\n",
                "# Fill in the line below: get test predictions\n",
                "preds_test = model.predict(final_X_test)\n",
                "\n",
                "# Check your answers\n",
                "step_4.b.check()\n",
                "Correct\n",
                "\n",
                "In [23]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_4.b.hint()\n",
                "step_4.b.solution()\n",
                "Solution:\n",
                "\n",
                "# Preprocess test data\n",
                "final_X_test = pd.DataFrame(final_imputer.transform(X_test))\n",
                "\n",
                "# Get test predictions\n",
                "preds_test = model.predict(final_X_test)\n",
                "Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition.\n",
                "\n",
                "In [24]:\n",
                "# Save test predictions to file\n",
                "output = pd.DataFrame({'Id': X_test.index,\n",
                "                       'SalePrice': preds_test})\n",
                "output.to_csv('submission.csv', index=False)\n",
                "X_test.index\n",
                "Out[24]:\n",
                "Int64Index([1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470,\n",
                "            ...\n",
                "            2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919],\n",
                "           dtype='int64', name='Id', length=1459)\n",
                "Submit your results\n",
                "Once you have successfully completed Step 4, you're ready to submit your results to the leaderboard! (You also learned how to do this in the previous exercise. If you need a reminder of how to do this, please use the instructions below.)\n",
                "\n",
                "First, you'll need to join the competition if you haven't already. So open a new window by clicking on this link. Then click on the Join Competition button.\n",
                "\n",
                "join competition image\n",
                "\n",
                "Next, follow the instructions below:\n",
                "\n",
                "Begin by clicking on the blue Save Version button in the top right corner of the window. This will generate a pop-up window.\n",
                "Ensure that the Save and Run All option is selected, and then click on the blue Save button.\n",
                "This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
                "Click on the Output tab on the right of the screen. Then, click on the file you would like to submit, and click on the blue Submit button to submit your results to the leaderboard.\n",
                "You have now successfully submitted to the competition!\n",
                "\n",
                "If you want to keep working to improve your performance, select the blue Edit button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n",
                "\n",
                "Keep going\n",
                "Move on to learn what categorical variables are, along with how to incorporate them into your machine learning models. Categorical variables are very common in real-world data, but you'll get an error if you try to plug them into your models without processing them first!\n",
                "\n",
                "Have questions or comments? Visit the Learn Discussion forum to chat with other Learners.#"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-68-a2a2089fb692>, line 1)",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-a2a2089fb692>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    This notebook is an exercise in the Intermediate Machine Learning course. You can reference the tutorial at this link.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "source": [
                "This notebook is an exercise in the Intermediate Machine Learning course. You can reference the tutorial at this link.\n",
                "\n",
                "By encoding categorical variables, you'll obtain your best results thus far!\n",
                "\n",
                "Setup\n",
                "The questions below will give you feedback on your work. Run the following cell to set up the feedback system.\n",
                "\n",
                "In [1]:\n",
                "# Set up code checking\n",
                "import os\n",
                "if not os.path.exists(\"../input/train.csv\"):\n",
                "    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n",
                "    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.ml_intermediate.ex3 import *\n",
                "print(\"Setup Complete\")\n",
                "Setup Complete\n",
                "In this exercise, you will work with data from the Housing Prices Competition for Kaggle Learn Users.\n",
                "\n",
                "Ames Housing dataset image\n",
                "\n",
                "Run the next code cell without changes to load the training and validation sets in X_train, X_valid, y_train, and y_valid. The test set is loaded in X_test.\n",
                "\n",
                "In [2]:\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Read the data\n",
                "X = pd.read_csv('../input/train.csv', index_col='Id') \n",
                "X_test = pd.read_csv('../input/test.csv', index_col='Id')\n",
                "\n",
                "# Remove rows with missing target, separate target from predictors\n",
                "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
                "y = X.SalePrice\n",
                "X.drop(['SalePrice'], axis=1, inplace=True)\n",
                "\n",
                "# To keep things simple, we'll drop columns with missing values\n",
                "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
                "print(cols_with_missing)\n",
                "X.drop(cols_with_missing, axis=1, inplace=True)\n",
                "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
                "#X_test.drop(cols_with_missing, axis=1)\n",
                "#print(X_test.isnull().sum())\n",
                "# Break off validation set from training data\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
                "                                                      train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "['LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n",
                "Use the next code cell to print the first five rows of the data.\n",
                "\n",
                "In [3]:\n",
                "X_train.head()\n",
                "Out[3]:\n",
                "MSSubClass\tMSZoning\tLotArea\tStreet\tLotShape\tLandContour\tUtilities\tLotConfig\tLandSlope\tNeighborhood\t...\tOpenPorchSF\tEnclosedPorch\t3SsnPorch\tScreenPorch\tPoolArea\tMiscVal\tMoSold\tYrSold\tSaleType\tSaleCondition\n",
                "Id\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
                "619\t20\tRL\t11694\tPave\tReg\tLvl\tAllPub\tInside\tGtl\tNridgHt\t...\t108\t0\t0\t260\t0\t0\t7\t2007\tNew\tPartial\n",
                "871\t20\tRL\t6600\tPave\tReg\tLvl\tAllPub\tInside\tGtl\tNAmes\t...\t0\t0\t0\t0\t0\t0\t8\t2009\tWD\tNormal\n",
                "93\t30\tRL\t13360\tPave\tIR1\tHLS\tAllPub\tInside\tGtl\tCrawfor\t...\t0\t44\t0\t0\t0\t0\t8\t2009\tWD\tNormal\n",
                "818\t20\tRL\t13265\tPave\tIR1\tLvl\tAllPub\tCulDSac\tGtl\tMitchel\t...\t59\t0\t0\t0\t0\t0\t7\t2008\tWD\tNormal\n",
                "303\t20\tRL\t13704\tPave\tIR1\tLvl\tAllPub\tCorner\tGtl\tCollgCr\t...\t81\t0\t0\t0\t0\t0\t1\t2006\tWD\tNormal\n",
                "5 rows × 60 columns\n",
                "\n",
                "Notice that the dataset contains both numerical and categorical variables. You'll need to encode the categorical data before training a model.\n",
                "\n",
                "To compare different models, you'll use the same score_dataset() function from the tutorial. This function reports the mean absolute error (MAE) from a random forest model.\n",
                "\n",
                "In [4]:\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "# function for comparing different approaches\n",
                "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
                "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "    model.fit(X_train, y_train)\n",
                "    preds = model.predict(X_valid)\n",
                "    return mean_absolute_error(y_valid, preds)\n",
                "Step 1: Drop columns with categorical data\n",
                "You'll get started with the most straightforward approach. Use the code cell below to preprocess the data in X_train and X_valid to remove columns with categorical data. Set the preprocessed DataFrames to drop_X_train and drop_X_valid, respectively.\n",
                "\n",
                "In [5]:\n",
                "# Fill in the lines below: drop columns in training and validation data\n",
                "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
                "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
                "\n",
                "# Check your answers\n",
                "step_1.check()\n",
                "Correct\n",
                "\n",
                "In [6]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_1.hint()\n",
                "#step_1.solution()\n",
                "Run the next code cell to get the MAE for this approach.\n",
                "\n",
                "In [7]:\n",
                "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
                "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
                "MAE from Approach 1 (Drop categorical variables):\n",
                "17837.82570776256\n",
                "Before jumping into ordinal encoding, we'll investigate the dataset. Specifically, we'll look at the 'Condition2' column. The code cell below prints the unique entries in both the training and validation sets.\n",
                "\n",
                "In [8]:\n",
                "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
                "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())\n",
                "Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
                "\n",
                "Unique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n",
                "Step 2: Ordinal encoding\n",
                "Part A\n",
                "\n",
                "If you now write code to:\n",
                "\n",
                "fit an ordinal encoder to the training data, and then\n",
                "use it to transform both the training and validation data,\n",
                "you'll get an error. Can you see why this is the case? (You'll need to use the above output to answer this question.)\n",
                "\n",
                "In [9]:\n",
                "# Check your answer (Run this code cell to receive credit!)\n",
                "step_2.a.check()\n",
                "Correct:\n",
                "\n",
                "Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use an ordinal encoder with scikit-learn, the code will throw an error.\n",
                "\n",
                "In [10]:\n",
                "#step_2.a.hint()\n",
                "This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.\n",
                "\n",
                "Run the code cell below to save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely ordinal encoded are stored in good_label_cols.\n",
                "\n",
                "In [11]:\n",
                "# All categorical columns\n",
                "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
                "\n",
                "# Columns that can be safely ordinal encoded\n",
                "good_label_cols = [col for col in object_cols if \n",
                "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
                "        \n",
                "# Problematic columns that will be dropped from the dataset\n",
                "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
                "        \n",
                "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
                "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
                "Categorical columns that will be ordinal encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
                "\n",
                "Categorical columns that will be dropped from the dataset: ['Functional', 'RoofMatl', 'Condition2']\n",
                "Part B\n",
                "\n",
                "Use the next code cell to ordinal encode the data in X_train and X_valid. Set the preprocessed DataFrames to label_X_train and label_X_valid, respectively.\n",
                "\n",
                "We have provided code below to drop the categorical columns in bad_label_cols from the dataset.\n",
                "You should ordinal encode the categorical columns in good_label_cols.\n",
                "In [12]:\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "# Drop categorical columns that will not be encoded\n",
                "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
                "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
                "\n",
                "# Apply ordinal encoder \n",
                "____ # Your code here\n",
                "oe = OrdinalEncoder()\n",
                "label_X_train[good_label_cols] = oe.fit_transform(X_train[good_label_cols])\n",
                "label_X_valid[good_label_cols] = oe.transform(X_valid[good_label_cols])\n",
                "# Check your answer\n",
                "step_2.b.check()\n",
                "Correct\n",
                "\n",
                "In [13]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_2.b.hint()\n",
                "#step_2.b.solution()\n",
                "Run the next code cell to get the MAE for this approach.\n",
                "\n",
                "In [14]:\n",
                "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
                "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
                "MAE from Approach 2 (Ordinal Encoding):\n",
                "17098.01649543379\n",
                "So far, you've tried two different approaches to dealing with categorical variables. And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n",
                "\n",
                "Soon, you'll try one-hot encoding. Before then, there's one additional topic we need to cover. Begin by running the next code cell without changes.\n",
                "\n",
                "In [15]:\n",
                "# Get number of unique entries in each column with categorical data\n",
                "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
                "d = dict(zip(object_cols, object_nunique))\n",
                "\n",
                "# Print number of unique entries by column, in ascending order\n",
                "sorted(d.items(), key=lambda x: x[1])\n",
                "Out[15]:\n",
                "[('Street', 2),\n",
                " ('Utilities', 2),\n",
                " ('CentralAir', 2),\n",
                " ('LandSlope', 3),\n",
                " ('PavedDrive', 3),\n",
                " ('LotShape', 4),\n",
                " ('LandContour', 4),\n",
                " ('ExterQual', 4),\n",
                " ('KitchenQual', 4),\n",
                " ('MSZoning', 5),\n",
                " ('LotConfig', 5),\n",
                " ('BldgType', 5),\n",
                " ('ExterCond', 5),\n",
                " ('HeatingQC', 5),\n",
                " ('Condition2', 6),\n",
                " ('RoofStyle', 6),\n",
                " ('Foundation', 6),\n",
                " ('Heating', 6),\n",
                " ('Functional', 6),\n",
                " ('SaleCondition', 6),\n",
                " ('RoofMatl', 7),\n",
                " ('HouseStyle', 8),\n",
                " ('Condition1', 9),\n",
                " ('SaleType', 9),\n",
                " ('Exterior1st', 15),\n",
                " ('Exterior2nd', 16),\n",
                " ('Neighborhood', 25)]\n",
                "Step 3: Investigating cardinality\n",
                "Part A\n",
                "\n",
                "The output above shows, for each column with categorical data, the number of unique values in the column. For instance, the 'Street' column in the training data has two unique values: 'Grvl' and 'Pave', corresponding to a gravel road and a paved road, respectively.\n",
                "\n",
                "We refer to the number of unique entries of a categorical variable as the cardinality of that categorical variable. For instance, the 'Street' variable has cardinality 2.\n",
                "\n",
                "Use the output above to answer the questions below.\n",
                "\n",
                "In [16]:\n",
                "# Fill in the line below: How many categorical variables in the training data\n",
                "# have cardinality greater than 10?\n",
                "high_cardinality_numcols = 3\n",
                "\n",
                "# Fill in the line below: How many columns are needed to one-hot encode the \n",
                "# 'Neighborhood' variable in the training data?\n",
                "num_cols_neighborhood = 25\n",
                "\n",
                "# Check your answers\n",
                "step_3.a.check()\n",
                "Correct\n",
                "\n",
                "In [17]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_3.a.hint()\n",
                "#step_3.a.solution()\n",
                "Part B\n",
                "\n",
                "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.\n",
                "\n",
                "As an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.\n",
                "\n",
                "If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?\n",
                "If we instead replace the column with the ordinal encoding, how many entries are added?\n",
                "Use your answers to fill in the lines below.\n",
                "\n",
                "In [18]:\n",
                "# Fill in the line below: How many entries are added to the dataset by \n",
                "# replacing the column with a one-hot encoding?\n",
                "OH_entries_added = 10_000_00-10000\n",
                "\n",
                "# Fill in the line below: How many entries are added to the dataset by\n",
                "# replacing the column with an ordinal encoding?\n",
                "label_entries_added = 0\n",
                "\n",
                "# Check your answers\n",
                "step_3.b.check()\n",
                "Correct\n",
                "\n",
                "In [19]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_3.b.hint()\n",
                "#step_3.b.solution()\n",
                "Next, you'll experiment with one-hot encoding. But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n",
                "\n",
                "Run the code cell below without changes to set low_cardinality_cols to a Python list containing the columns that will be one-hot encoded. Likewise, high_cardinality_cols contains a list of categorical columns that will be dropped from the dataset.\n",
                "\n",
                "In [20]:\n",
                "# Columns that will be one-hot encoded\n",
                "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
                "\n",
                "# Columns that will be dropped from the dataset\n",
                "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
                "\n",
                "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
                "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n",
                "Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
                "\n",
                "Categorical columns that will be dropped from the dataset: ['Exterior2nd', 'Exterior1st', 'Neighborhood']\n",
                "Step 4: One-hot encoding\n",
                "Use the next code cell to one-hot encode the data in X_train and X_valid. Set the preprocessed DataFrames to OH_X_train and OH_X_valid, respectively.\n",
                "\n",
                "The full list of categorical columns in the dataset can be found in the Python list object_cols.\n",
                "You should only one-hot encode the categorical columns in low_cardinality_cols. All other categorical columns should be dropped from the dataset.\n",
                "In [21]:\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "# Use as many lines of code as you need!\n",
                "\n",
                "OHE = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
                "\n",
                "OH_cols_train = pd.DataFrame(OHE.fit_transform(X_train[low_cardinality_cols])) # Your code here\n",
                "OH_cols_valid = pd.DataFrame(OHE.transform(X_valid[low_cardinality_cols])) # Your code here\n",
                "\n",
                "OH_cols_train.index = X_train.index\n",
                "OH_cols_valid.index = X_valid.index\n",
                "\n",
                "num_X_train = X_train.drop(object_cols,axis=1)\n",
                "num_X_valid= X_valid.drop(object_cols,axis=1)\n",
                "\n",
                "OH_X_train = pd.concat([num_X_train,OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid,OH_cols_valid], axis=1)\n",
                "# Check your answer\n",
                "step_4.check()\n",
                "Correct\n",
                "\n",
                "In [22]:\n",
                "# Lines below will give you a hint or solution code\n",
                "#step_4.hint()\n",
                "step_4.solution()\n",
                "Solution:\n",
                "\n",
                "# Apply one-hot encoder to each column with categorical data\n",
                "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
                "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
                "\n",
                "# One-hot encoding removed index; put it back\n",
                "OH_cols_train.index = X_train.index\n",
                "OH_cols_valid.index = X_valid.index\n",
                "\n",
                "# Remove categorical columns (will replace with one-hot encoding)\n",
                "num_X_train = X_train.drop(object_cols, axis=1)\n",
                "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
                "\n",
                "# Add one-hot encoded columns to numerical features\n",
                "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
                "Run the next code cell to get the MAE for this approach.\n",
                "\n",
                "In [23]:\n",
                "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
                "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
                "MAE from Approach 3 (One-Hot Encoding):\n",
                "17525.345719178084\n",
                "Generate test predictions and submit your results\n",
                "After you complete Step 4, if you'd like to use what you've learned to submit your results to the leaderboard, you'll need to preprocess the test data before generating predictions.\n",
                "\n",
                "This step is completely optional, and you do not need to submit results to the leaderboard to successfully complete the exercise.\n",
                "\n",
                "Check out the previous exercise if you need help with remembering how to join the competition or save your results to CSV. Once you have generated a file with your results, follow the instructions below:\n",
                "\n",
                "Begin by clicking on the Save Version button in the top right corner of the window. This will generate a pop-up window.\n",
                "Ensure that the Save and Run All option is selected, and then click on the Save button.\n",
                "This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the Save Version button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (...) to the right of the most recent version, and select Open in Viewer. This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
                "Click on the Output tab on the right of the screen. Then, click on the file you would like to submit, and click on the blue Submit button to submit your results to the leaderboard.\n",
                "You have now successfully submitted to the competition!\n",
                "\n",
                "If you want to keep working to improve your performance, select the Edit button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n",
                "\n",
                "In [24]:\n",
                "# (Optional) Your code here\n",
                "'''from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "# Drop categorical columns that will not be encoded\n",
                "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
                "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
                "\n",
                "# Apply ordinal encoder \n",
                "____ # Your code here\n",
                "oe = OrdinalEncoder()\n",
                "label_X_train[good_label_cols] = oe.fit_transform(X_train[good_label_cols])\n",
                "label_X_valid[good_label_cols] = oe.transform(X_valid[good_label_cols])\n",
                "\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "model.fit(label_X_train, y_train)\n",
                "preds = model.predict(label_X_valid)\n",
                "mae = mean_absolute_error(y_valid, preds)\n",
                "\n",
                "X_new=X_test.copy()\n",
                "X_new=X_new.drop(bad_label_cols,axis=1)\n",
                "\n",
                "\n",
                "num_X = X_new.drop(good_label_cols, axis=1)\n",
                "\n",
                "from sklearn.impute import SimpleImputer\n",
                "si = SimpleImputer()\n",
                "imputed_num_X = pd.DataFrame(si.fit_transform(num_X)) #why fit here? just transform didnt work\n",
                "imputed_num_X.columns = num_X.columns\n",
                "\n",
                "label_X_test = X_test.copy()\n",
                "\n",
                "label_X_test = label_X_test.select_dtypes(exclude=['int64','float64'])\n",
                "\n",
                "label_X_test = label_X_test.drop(bad_label_cols, axis=1)\n",
                "label_X_test.isnull().sum()\n",
                "#label_X_test[good_label_cols] = oe.transform(label_X_test[good_label_cols])'''\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
                "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
                "\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
                "model.fit(drop_X_train, y_train)\n",
                "preds = model.predict(drop_X_valid)\n",
                "mae= mean_absolute_error(y_valid, preds)\n",
                "\n",
                "drop_X_test = X_test.select_dtypes(exclude=['object'])\n",
                "\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "si = SimpleImputer()\n",
                "\n",
                "idXt = pd.DataFrame(si.fit_transform(drop_X_test))\n",
                "\n",
                "idXt.columns = drop_X_test.columns\n",
                "\n",
                "predict_test = model.predict(idXt)\n",
                "\n",
                "output = pd.DataFrame ( {'Id':X_test.index, 'SalePrice':predict_test})\n",
                "output.to_csv('submission_nothappy_droppedcatvar_NaNinCatVar_cantimpute.csv', index=False)\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "IndentationError",
                    "evalue": "unindent does not match any outer indentation level (<tokenize>, line 193)",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m193\u001b[0m\n\u001b[0;31m    ('Utilities', 2),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.4 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "5bb92a85c9a1f15da6b689aacf75061d89825faba8bdf21749792174943e74b5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}